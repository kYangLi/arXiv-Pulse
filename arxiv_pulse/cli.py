#!/usr/bin/env python3
"""
arXiv Pulse - ç®€åŒ–ç‰ˆå‘½ä»¤è¡Œç•Œé¢
æ ¸å¿ƒåŠŸèƒ½ï¼šåˆå§‹åŒ–ã€æ›´æ–°åŒæ­¥ã€æ™ºèƒ½æœç´¢ã€æœ€è¿‘è®ºæ–‡æŠ¥å‘Š
"""

import json
import sys
from datetime import datetime
from pathlib import Path

import click
import openai
import questionary

from arxiv_pulse.__version__ import __version__
from arxiv_pulse.arxiv_crawler import ArXivCrawler
from arxiv_pulse.banner import generate_banner_title, print_banner, print_banner_custom
from arxiv_pulse.config import Config
from arxiv_pulse.environment import setup_environment
from arxiv_pulse.output_manager import OutputLevel, output
from arxiv_pulse.report_generator import ReportGenerator
from arxiv_pulse.research_fields import DEFAULT_BANNER_FIELDS, RESEARCH_FIELDS
from arxiv_pulse.search_engine import SearchEngine, SearchFilter
from arxiv_pulse.summarizer import PaperSummarizer
from arxiv_pulse.utils import get_workday_cutoff, parse_time_range


def sync_papers(years_back=1, summarize=False, force=False, arxiv_max_results=None):
    """åŒæ­¥è®ºæ–‡ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    crawler = ArXivCrawler()
    summarizer = PaperSummarizer()

    if arxiv_max_results is None:
        arxiv_max_results = Config.ARXIV_MAX_RESULTS

    sync_description = crawler.get_sync_description(years_back, force)

    mode_text = "å¼ºåˆ¶åŒæ­¥" if force else "åŒæ­¥ç¼ºå¤±è®ºæ–‡"
    click.echo(f"æ­£åœ¨{mode_text}ï¼ˆ{sync_description}ï¼Œæœ€å¤§ {arxiv_max_results} ç¯‡ï¼‰...")
    click.echo("=" * 50)

    click.echo("1. æ­£åœ¨åŒæ­¥æœç´¢æŸ¥è¯¢...")
    sync_result = crawler.sync_all_queries(years_back=years_back, force=force, arxiv_max_results=arxiv_max_results)
    result_text = "å¤„ç†äº†" if force else "æ·»åŠ äº†"
    click.echo(f"   ä»æŸ¥è¯¢{result_text} {sync_result['total_new_papers']} ç¯‡è®ºæ–‡")

    click.echo("2. æ­£åœ¨åŒæ­¥é‡è¦è®ºæ–‡...")
    important_result = crawler.sync_important_papers()
    click.echo(f"   æ·»åŠ äº† {important_result['added']} ç¯‡é‡è¦è®ºæ–‡")
    if important_result["errors"]:
        click.echo(f"   é”™è¯¯: {len(important_result['errors'])}")

    total_new = sync_result["total_new_papers"] + important_result["added"]
    if summarize and total_new > 0:
        click.echo("3. æ­£åœ¨æ€»ç»“æ–°è®ºæ–‡...")
        summarize_result = summarizer.summarize_pending_papers(limit=min(64, total_new))
        click.echo(f"   å·²æ€»ç»“ {summarize_result['successful']} ç¯‡è®ºæ–‡")
    elif total_new > 0:
        click.echo("3. è·³è¿‡è®ºæ–‡æ€»ç»“")
    else:
        click.echo("3. æ²¡æœ‰æ–°è®ºæ–‡éœ€è¦æ€»ç»“")

    crawl_stats = crawler.get_crawler_stats()
    summary_stats = summarizer.get_summary_stats()

    click.echo("\n" + "=" * 50)
    click.echo("åŒæ­¥å®Œæˆï¼")
    click.echo(f"æ€»å…±{result_text}è®ºæ–‡: {total_new}")
    click.echo(f"æ•°æ®åº“ç°æœ‰ {crawl_stats['total_papers']} ç¯‡è®ºæ–‡")
    click.echo(f"å·²æ€»ç»“: {summary_stats['summarized_papers']} ({summary_stats['summarization_rate']:.1%})")

    return {
        "crawler": crawler,
        "summarizer": summarizer,
        "sync_result": sync_result,
        "important_result": important_result,
        "stats": {"crawl_stats": crawl_stats, "summary_stats": summary_stats},
        "force_mode": force,
    }


def generate_report(paper_limit=64, days_back=2, summarize=True, max_summarize=10, cache=True):
    """ç”Ÿæˆæœ€è¿‘è®ºæ–‡çš„æŠ¥å‘Šï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    reporter = ReportGenerator()
    reporter.use_cache = cache

    original_limit = Config.REPORT_MAX_PAPERS
    Config.REPORT_MAX_PAPERS = paper_limit

    try:
        with reporter.db.get_session() as session:
            from arxiv_pulse.models import Paper

            cutoff = get_workday_cutoff(days_back)
            recent_papers = (
                session.query(Paper)
                .filter(Paper.published >= cutoff)
                .order_by(Paper.published.desc())
                .limit(paper_limit)
                .all()
            )

            summarizer = PaperSummarizer()

            category_counts = {}
            for paper in recent_papers:
                if paper.categories is not None:
                    for cat in paper.categories.split():
                        category_counts[cat] = category_counts.get(cat, 0) + 1

            top_categories = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5])

            crawler = ArXivCrawler()
            crawl_stats = crawler.get_crawler_stats()
            summary_stats = summarizer.get_summary_stats()

            report_data = {
                "stats": {
                    "total_recent": len(recent_papers),
                    "days_back": days_back,
                    "report_type": "recent",
                    "date_generated": datetime.now().isoformat(),
                    "database_stats": {
                        "total_papers": crawl_stats["total_papers"],
                        "summarized_papers": summary_stats["summarized_papers"],
                    },
                    "top_categories": top_categories,
                    "summarize": summarize,
                    "max_summarize": max_summarize,
                },
                "papers": recent_papers,
            }

        files = []
        md_file = reporter.save_markdown_report(report_data)
        if md_file:
            files.append(md_file)

        csv_file = reporter.save_csv_report(report_data)
        if csv_file:
            files.append(csv_file)

        return files
    finally:
        Config.REPORT_MAX_PAPERS = original_limit


def generate_search_report(query, search_terms, papers, paper_limit=64, summarize=True, max_summarize=10, cache=True):
    """ç”Ÿæˆæœç´¢ç»“æœçš„æŠ¥å‘Šï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    reporter = ReportGenerator()
    reporter.use_cache = cache

    if not papers:
        output.info("æœªæ‰¾åˆ°è®ºæ–‡ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        return []

    original_limit = Config.REPORT_MAX_PAPERS
    Config.REPORT_MAX_PAPERS = paper_limit

    try:
        summarizer = PaperSummarizer()

        category_counts = {}
        for paper in papers:
            if paper.categories is not None:
                for cat in paper.categories.split():
                    category_counts[cat] = category_counts.get(cat, 0) + 1

        top_categories = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5])

        crawler = ArXivCrawler()
        crawl_stats = crawler.get_crawler_stats()
        summary_stats = summarizer.get_summary_stats()

        report_data = {
            "stats": {
                "total_found": len(papers),
                "original_query": query,
                "search_terms": search_terms,
                "report_type": "search",
                "date_generated": datetime.now().isoformat(),
                "database_stats": {
                    "total_papers": crawl_stats["total_papers"],
                    "summarized_papers": summary_stats["summarized_papers"],
                },
                "top_categories": top_categories,
                "summarize": summarize,
                "max_summarize": max_summarize,
            },
            "papers": papers,
        }

        files = []
        md_file = reporter.save_markdown_report(report_data)
        if md_file:
            files.append(md_file)

        csv_file = reporter.save_csv_report(report_data)
        if csv_file:
            files.append(csv_file)

        return files
    finally:
        Config.REPORT_MAX_PAPERS = original_limit


def interactive_configuration():
    """äº¤äº’å¼é…ç½® arXiv Pulse"""
    config = {}

    click.echo("\n" + "=" * 60)
    click.echo("arXiv Pulse äº¤äº’å¼é…ç½®å‘å¯¼")
    click.echo("=" * 60)

    click.echo("\nğŸ”§ AI API é…ç½®")
    click.echo("-" * 40)

    ai_base_url = click.prompt("AI API Base URL", default="https://llmapi.paratera.com", show_default=True)
    config["AI_BASE_URL"] = ai_base_url

    ai_api_key = click.prompt(
        "è¯·è¾“å…¥ AI API å¯†é’¥ (ç•™ç©ºåˆ™è·³è¿‡ï¼Œç¨åå¯åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ )", default="", show_default=False, hide_input=True
    )
    if ai_api_key:
        config["AI_API_KEY"] = ai_api_key
        available_models = []
        try:
            click.echo("æ­£åœ¨æŸ¥è¯¢å¯ç”¨æ¨¡å‹...")
            client = openai.OpenAI(base_url=ai_base_url, api_key=ai_api_key)
            models_response = client.models.list()
            available_models = [model.id for model in models_response.data]
            click.echo(f"âœ… æ‰¾åˆ° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹")
        except Exception as e:
            click.echo(f"âš ï¸  æ— æ³•æŸ¥è¯¢æ¨¡å‹åˆ—è¡¨: {e}")
            click.echo("   å°†ä½¿ç”¨é»˜è®¤æ¨¡å‹é€‰é¡¹")
            available_models = ["DeepSeek-V3.2-Thinking", "gpt-3.5-turbo", "gpt-4-turbo"]
    else:
        click.echo("âš ï¸  æœªæä¾› API å¯†é’¥ï¼ŒAI æ€»ç»“å’Œç¿»è¯‘åŠŸèƒ½å°†å—é™")
        click.echo("   æ‚¨å¯ä»¥ç¨ååœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ  AI_API_KEY è®¾ç½®")
        config["AI_API_KEY"] = "your_api_key_here"
        available_models = ["DeepSeek-V3.2-Thinking", "gpt-3.5-turbo", "gpt-4-turbo"]

    if available_models:
        click.echo("\nå¯ç”¨æ¨¡å‹åˆ—è¡¨:")

        choices = []
        for model in available_models:
            choices.append(questionary.Choice(title=model, value=model))

        choices.append(questionary.Choice(title="[è‡ªå®šä¹‰è¾“å…¥] - è¾“å…¥å…¶ä»–æ¨¡å‹åç§°", value="__custom_input__"))

        selected_model = questionary.select(
            "è¯·é€‰æ‹©AIæ¨¡å‹ï¼ˆä½¿ç”¨ä¸Šä¸‹ç®­å¤´å¯¼èˆªï¼Œå›è½¦ç¡®è®¤ï¼‰:", choices=choices, instruction="(ä¸Šä¸‹ç®­å¤´å¯¼èˆªï¼Œå›è½¦ç¡®è®¤)"
        ).ask()

        if selected_model == "__custom_input__":
            ai_model = click.prompt("è¯·è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°", default="DeepSeek-V3.2-Thinking", show_default=True)
            click.echo(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {ai_model}")
        else:
            ai_model = selected_model
            click.echo(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {ai_model}")
    else:
        ai_model = click.prompt("AI æ¨¡å‹åç§°", default="DeepSeek-V3.2-Thinking", show_default=True)

    config["AI_MODEL"] = ai_model

    click.echo("\nğŸ“Š çˆ¬è™«é…ç½®")
    click.echo("-" * 40)

    arxiv_max_results = click.prompt("arXiv API æœ€å¤§è¿”å›è®ºæ–‡æ•°", default=10000, type=int, show_default=True)
    config["ARXIV_MAX_RESULTS"] = str(arxiv_max_results)

    years_back = click.prompt("åˆå§‹åŒæ­¥å›æº¯çš„å¹´æ•°", default=5, type=int, show_default=True)
    config["YEARS_BACK"] = str(years_back)

    click.echo("\nğŸ¯ é€‰æ‹©æ‚¨çš„ç ”ç©¶é¢†åŸŸ")
    click.echo("-" * 40)
    click.echo("è¯·ä½¿ç”¨ä¸Šä¸‹ç®­å¤´å¯¼èˆªï¼Œç©ºæ ¼é”®é€‰æ‹©/å–æ¶ˆï¼Œå›è½¦ç¡®è®¤ï¼ˆå¯å¤šé€‰ï¼‰ï¼š")

    research_fields = RESEARCH_FIELDS

    choices = []
    for key, field in research_fields.items():
        title = f"[{field['name']}] - {field['description']}"
        choices.append(
            questionary.Choice(
                title=title,
                value=key,
                checked=False,
            )
        )

    choices.insert(0, questionary.Choice(title="[å…¨é€‰] - é€‰æ‹©æ‰€æœ‰ç ”ç©¶é¢†åŸŸ", value="__select_all__", checked=False))

    selected_keys = questionary.checkbox(
        "è¯·é€‰æ‹©æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶é¢†åŸŸï¼š",
        choices=choices,
        instruction="(ç©ºæ ¼é”®åˆ‡æ¢é€‰æ‹©ï¼Œå›è½¦ç¡®è®¤)",
        validate=lambda selected: len(selected) > 0 or "è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç ”ç©¶é¢†åŸŸ",
    ).ask()

    if not selected_keys:
        click.echo("âŒ æœªé€‰æ‹©ä»»ä½•ç ”ç©¶é¢†åŸŸï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
        selected_keys = ["condensed_matter", "dft", "machine_learning"]

    selected_queries = []
    selected_field_names = []

    if "__select_all__" in selected_keys:
        for field in research_fields.values():
            selected_queries.append(field["query"])
            selected_field_names.append(field["name"])
        click.echo("âœ… å·²é€‰æ‹©å…¨éƒ¨ç ”ç©¶é¢†åŸŸ")
    else:
        for key in selected_keys:
            if key in research_fields:
                field = research_fields[key]
                selected_queries.append(field["query"])
                selected_field_names.append(field["name"])
                click.echo(f"âœ… å·²é€‰æ‹©: {field['name']}")
            else:
                click.echo(f"âš ï¸  æœªçŸ¥çš„é¢†åŸŸID: {key}")

    if not selected_queries:
        click.echo("âš ï¸  æœªé€‰æ‹©ä»»ä½•é¢†åŸŸï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        selected_queries = [
            research_fields["condensed_matter"]["query"],
            research_fields["dft"]["query"],
            research_fields["machine_learning"]["query"],
        ]
        selected_field_names = [
            research_fields["condensed_matter"]["name"],
            research_fields["dft"]["name"],
            research_fields["machine_learning"]["name"],
        ]

    config["SEARCH_QUERIES"] = "; ".join(selected_queries)
    config["_SELECTED_FIELD_NAMES"] = selected_field_names

    num_selected_fields = len(selected_field_names)
    click.echo(f"\nğŸ“Š æ™ºèƒ½å»ºè®®ï¼ˆåŸºäºæ‚¨é€‰æ‹©çš„ {num_selected_fields} ä¸ªç ”ç©¶é¢†åŸŸï¼‰")
    click.echo("-" * 40)

    recommended_max_results = 0
    if num_selected_fields <= 6:
        click.echo("âœ… æ‚¨é€‰æ‹©äº†å°‘é‡é¢†åŸŸï¼Œä¿æŒé»˜è®¤é…ç½®å³å¯ã€‚")
        recommended_max_results = 10000
    elif num_selected_fields <= 10:
        recommended_max_results = 4000
        click.echo("âš ï¸  æ‚¨é€‰æ‹©äº†ä¸­ç­‰æ•°é‡é¢†åŸŸï¼Œå»ºè®®è°ƒæ•´ ARXIV_MAX_RESULTSï¼š")
        click.echo(f"   - arXiv API æœ€å¤§è¿”å›è®ºæ–‡æ•°: {recommended_max_results}")
    else:
        recommended_max_results = 1000
        click.echo(f"âš ï¸  æ‚¨é€‰æ‹©äº†å¤§é‡é¢†åŸŸ ({num_selected_fields}ä¸ª)ï¼Œå¼ºçƒˆå»ºè®®è°ƒæ•´ ARXIV_MAX_RESULTSï¼š")
        click.echo(f"   - arXiv API æœ€å¤§è¿”å›è®ºæ–‡æ•°: {recommended_max_results}")
        click.echo("   - æ³¨æ„ï¼šåŒæ­¥å¤§é‡é¢†åŸŸå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’Œæ›´å¤šå­˜å‚¨ç©ºé—´ã€‚")

    if num_selected_fields > 6:
        if click.confirm("\nğŸ’¡ æ˜¯å¦åº”ç”¨ä¸Šè¿°å»ºè®®è°ƒæ•´ ARXIV_MAX_RESULTSï¼Ÿ", default=True):
            config["ARXIV_MAX_RESULTS"] = str(recommended_max_results)
            click.echo(f"âœ… å·²åº”ç”¨å»ºè®®é…ç½®ï¼šARXIV_MAX_RESULTS={recommended_max_results}")
        else:
            click.echo("â„¹ï¸  ä¿æŒæ‚¨åŸæœ‰çš„ ARXIV_MAX_RESULTS é…ç½®ã€‚")

    click.echo("\nğŸ“„ æŠ¥å‘Šé…ç½®")
    click.echo("-" * 40)

    report_max_papers = click.prompt("æ¯ä»½æŠ¥å‘Šæ˜¾ç¤ºçš„æœ€å¤§è®ºæ–‡æ•°", default=64, type=int, show_default=True)
    config["REPORT_MAX_PAPERS"] = str(report_max_papers)

    click.echo("\nâœ… é…ç½®å®Œæˆï¼")
    return config, int(years_back)


@click.group(context_settings={"help_option_names": ["-h", "--help"]})
@click.option("--verbose", "-v", is_flag=True, help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºï¼ˆåŒ…æ‹¬è°ƒè¯•ä¿¡æ¯ï¼‰")
@click.version_option(version=__version__, prog_name="arXiv Pulse")
def cli(verbose):
    """arXiv Pulse: æ™ºèƒ½arXivæ–‡çŒ®è¿½è¸ªå’Œåˆ†æç³»ç»Ÿ"""
    if verbose:
        output.set_min_level(OutputLevel.DEBUG)


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
def init(directory):
    """åˆå§‹åŒ–ç›®å½•å¹¶åŒæ­¥å†å²è®ºæ–‡"""
    directory = Path(directory).resolve()

    (directory / "data").mkdir(exist_ok=True)
    (directory / "reports").mkdir(exist_ok=True)

    env_file = directory / ".env"
    custom_banner_fields = None

    if not env_file.exists():
        config, years_back = interactive_configuration()

        custom_banner_fields = config.get("_SELECTED_FIELD_NAMES", [])

        template_file = Path(__file__).parent / ".ENV.TEMPLATE"
        if not template_file.exists():
            click.echo(f"âŒ æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶: {template_file}")
            click.echo("è¯·ç¡®ä¿ .ENV.TEMPLATE æ–‡ä»¶å­˜åœ¨äº arxiv_pulse ç›®å½•ä¸­")
            return

        env_content = template_file.read_text(encoding="utf-8")

        timestamp_comment = f"# ç”±äº¤äº’å¼é…ç½®å‘å¯¼äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ç”Ÿæˆ\n"
        lines = env_content.split("\n")
        if lines and lines[0].startswith("#"):
            lines.insert(1, timestamp_comment)
        else:
            lines.insert(0, timestamp_comment)
        env_content = "\n".join(lines)

        lines = env_content.split("\n")

        for i, line in enumerate(lines):
            if line.strip().startswith("AI_API_KEY="):
                lines[i] = f"AI_API_KEY={config.get('AI_API_KEY', 'your_api_key_here')}"
                break

        for i, line in enumerate(lines):
            if line.strip().startswith("AI_MODEL="):
                lines[i] = f"AI_MODEL={config.get('AI_MODEL', 'DeepSeek-V3.2-Thinking')}"
                break

        for i, line in enumerate(lines):
            if line.strip().startswith("AI_BASE_URL="):
                lines[i] = f"AI_BASE_URL={config.get('AI_BASE_URL', 'https://llmapi.paratera.com')}"
                break

        for i, line in enumerate(lines):
            if line.strip().startswith("ARXIV_MAX_RESULTS="):
                lines[i] = f"ARXIV_MAX_RESULTS={config.get('ARXIV_MAX_RESULTS', '10000')}"
                break

        default_search_queries = 'condensed matter physics AND cat:cond-mat.*; (ti:"density functional" OR abs:"density functional") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci OR cat:physics.chem-ph); (ti:"machine learning" OR abs:"machine learning") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci OR cat:physics.chem-ph)'
        for i, line in enumerate(lines):
            if line.strip().startswith("SEARCH_QUERIES="):
                lines[i] = f"SEARCH_QUERIES={config.get('SEARCH_QUERIES', default_search_queries)}"
                break

        for i, line in enumerate(lines):
            if line.strip().startswith("REPORT_MAX_PAPERS="):
                lines[i] = f"REPORT_MAX_PAPERS={config.get('REPORT_MAX_PAPERS', '64')}"
                break

        for i, line in enumerate(lines):
            if line.strip().startswith("YEARS_BACK="):
                lines[i] = f"YEARS_BACK={config.get('YEARS_BACK', '5')}"
                break

        env_content = "\n".join(lines)

        env_file.write_text(env_content)
        click.echo(f"\nâœ… å·²åœ¨ {directory} åˆ›å»º .env é…ç½®æ–‡ä»¶")

    else:
        click.echo(f".env æ–‡ä»¶å·²å­˜åœ¨äº {directory}")
        years_back = Config.YEARS_BACK

    important_file = directory / Config.IMPORTANT_PAPERS_FILE
    important_file.parent.mkdir(parents=True, exist_ok=True)
    if not important_file.exists():
        important_file.write_text("# åœ¨æ­¤æ·»åŠ é‡è¦è®ºæ–‡çš„arXiv IDï¼Œæ¯è¡Œä¸€ä¸ª\n")
        click.echo(f"âœ… å·²åˆ›å»ºé‡è¦è®ºæ–‡æ–‡ä»¶: {important_file}")

    if not setup_environment(directory):
        click.echo("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        sys.exit(1)

    click.echo("\n" + "=" * 60)
    click.echo("å‡†å¤‡åŒæ­¥æ•°æ®åº“")
    click.echo("=" * 60)
    click.echo(f"å³å°†å¼€å§‹åˆå§‹åŒæ­¥ï¼Œå›æº¯ {years_back} å¹´å†å²è®ºæ–‡...")
    click.echo("è¿™å¯èƒ½ä¼šèŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ‚¨é€‰æ‹©çš„é¢†åŸŸæ•°é‡ã€‚")
    click.echo("æ‚¨å¯ä»¥åœ¨ä»»ä½•æ—¶å€™æŒ‰ Ctrl+C ä¸­æ–­åŒæ­¥ã€‚")

    if not click.confirm("\nğŸš€ ç¡®è®¤å¼€å§‹åŒæ­¥æ•°æ®åº“å—ï¼Ÿ", default=True):
        click.echo("âŒ å·²å–æ¶ˆåŒæ­¥")
        sys.exit(0)

    click.echo(f"\nâ³ å¼€å§‹åˆå§‹åŒæ­¥ï¼Œå›æº¯ {years_back} å¹´å†å²è®ºæ–‡...")
    sync_result = sync_papers(years_back=years_back, summarize=False)

    if custom_banner_fields:
        banner_title = custom_banner_fields[:4]
    else:
        banner_title = generate_banner_title(env_file)
    print_banner_custom(banner_title)

    click.echo("\nğŸ‰ arXiv Pulse åˆå§‹åŒ–å®Œæˆï¼")
    click.echo("\nğŸ“ æ–‡ä»¶ä½ç½®ï¼š")
    click.echo(f"  é…ç½®æ–‡ä»¶: {env_file}")
    click.echo(f"  æ•°æ®åº“: {directory}/data/arxiv_papers.db")
    click.echo(f"  æŠ¥å‘Šç›®å½•: {directory}/reports/")
    click.echo("\nğŸš€ ä¸‹ä¸€æ­¥ï¼š")
    click.echo(f"  1. è¿è¡Œ 'pulse sync {directory}' æ›´æ–°æœ€æ–°è®ºæ–‡")
    click.echo(f"  2. è¿è¡Œ 'pulse search \"å…³é”®è¯\" {directory}' æœç´¢è®ºæ–‡")
    click.echo(f"  3. è¿è¡Œ 'pulse recent {directory}' æŸ¥çœ‹æœ€è¿‘è®ºæ–‡æŠ¥å‘Š")
    click.echo(f"  4. ç¼–è¾‘ {important_file} æ·»åŠ é‡è¦è®ºæ–‡")


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--years-back", type=int, default=None, help="åŒæ­¥å›æº¯çš„å¹´æ•°ï¼ˆé»˜è®¤ï¼š5å¹´ï¼‰")
@click.option("--force", is_flag=True, default=False, help="å¼ºåˆ¶åŒæ­¥ï¼šç»§ç»­æŸ¥è¯¢ï¼Œè·³è¿‡å·²å­˜åœ¨çš„è®ºæ–‡")
@click.option(
    "--arxiv-max-results", type=int, default=None, help="arXiv API æœ€å¤§è¿”å›è®ºæ–‡æ•°ï¼ˆé»˜è®¤ï¼šARXIV_MAX_RESULTSé…ç½®ï¼‰"
)
def sync(directory, years_back, force, arxiv_max_results):
    """åŒæ­¥æœ€æ–°è®ºæ–‡åˆ°æ•°æ®åº“

    æ™®é€šæ¨¡å¼ï¼ˆæ—  --forceï¼‰: æŒ‰æ—¶é—´ä»è¿‘åˆ°æ—©åŒæ­¥ï¼Œé‡åˆ°å·²å­˜åœ¨çš„è®ºæ–‡ç«‹å³åœæ­¢ã€‚
    å¼ºåˆ¶æ¨¡å¼ï¼ˆ--forceï¼‰: ç»§ç»­æŸ¥è¯¢ï¼Œè·³è¿‡å·²å­˜åœ¨çš„è®ºæ–‡ï¼Œç”¨äºæ‰©å±•æ•°æ®åº“ã€‚

    æ³¨æ„: æ— è®ºæ˜¯å¦ä½¿ç”¨ --forceï¼Œéƒ½ä¸ä¼šä¸‹è½½å·²å­˜åœ¨çš„è®ºæ–‡ã€‚
    """
    directory = Path(directory).resolve()
    click.echo(f"æ­£åœ¨åŒæ­¥ arXiv Pulse äº {directory}")

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    if years_back is None:
        years_back = Config.YEARS_BACK
        click.echo(f"ä½¿ç”¨é»˜è®¤å›æº¯å¹´æ•°: {years_back} å¹´")

    if arxiv_max_results is None:
        arxiv_max_results = Config.ARXIV_MAX_RESULTS
        click.echo(f"ä½¿ç”¨ ARXIV_MAX_RESULTS é…ç½®: {arxiv_max_results}")

    sync_result = sync_papers(years_back=years_back, summarize=False, force=force, arxiv_max_results=arxiv_max_results)

    click.echo("\n" + "=" * 50)
    click.echo("åŒæ­¥å®Œæˆï¼æ•°æ®åº“å·²æ›´æ–°ã€‚")


@cli.command()
@click.argument("query")
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--limit", default=64, help="è¿”å›ç»“æœçš„æœ€å¤§æ•°é‡ï¼ˆé»˜è®¤ï¼š64ï¼‰")
@click.option("--update/--no-update", default=False, help="æœç´¢å‰æ˜¯å¦æ›´æ–°æ•°æ®åº“ï¼ˆé»˜è®¤ï¼šå¦ï¼Œæ˜¯åˆ™ä½¿ç”¨YEARS_BACKé…ç½®ï¼‰")
@click.option(
    "--time-range", "-t", default="0", help="æœç´¢æ—¶é—´èŒƒå›´ï¼Œå¦‚'1y'=1å¹´ã€'6m'=6ä¸ªæœˆã€'30d'=30å¤©ï¼ˆé»˜è®¤ï¼š0ï¼Œè¡¨ç¤ºä¸é™åˆ¶ï¼‰"
)
@click.option("--categories", "-c", multiple=True, help="åŒ…å«çš„åˆ†ç±»ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰")
@click.option("--authors", "-a", multiple=True, help="ä½œè€…å§“åï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰")
@click.option(
    "--sort-by",
    type=click.Choice(["published", "relevance_score", "title", "updated"]),
    default="published",
    help="æ’åºå­—æ®µ",
)
@click.option("--no-cache", is_flag=True, default=False, help="ç¦ç”¨å›¾ç‰‡URLç¼“å­˜")
def search(
    query,
    directory,
    limit,
    update,
    time_range,
    categories,
    authors,
    sort_by,
    no_cache,
):
    """æ™ºèƒ½æœç´¢è®ºæ–‡ï¼ˆæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢å’ŒåŸºæœ¬è¿‡æ»¤ï¼‰"""
    directory = Path(directory).resolve()

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    crawler = ArXivCrawler()

    if update:
        years_back = Config.YEARS_BACK
        sync_result = sync_papers(years_back=years_back, summarize=False, force=False)

    click.echo(f"\næ­£åœ¨æœç´¢: '{query}'")
    click.echo("=" * 50)

    search_terms = [query]

    if Config.AI_API_KEY:
        try:
            client = openai.OpenAI(api_key=Config.AI_API_KEY, base_url=Config.AI_BASE_URL)

            ai_prompt = f"""
            ç”¨æˆ·æ­£åœ¨æœç´¢arXivç‰©ç†/è®¡ç®—ææ–™ç§‘å­¦è®ºæ–‡ï¼ŒæŸ¥è¯¢æ˜¯: "{query}"
            
            è¯·å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºé€‚åˆarXivæœç´¢çš„å…³é”®è¯æˆ–çŸ­è¯­ã€‚
            
            é‡è¦è§„åˆ™ï¼š
            1. å¦‚æœæŸ¥è¯¢å·²ç»æ˜¯æ˜ç¡®çš„æœç´¢è¯ï¼ˆå¦‚"DeepH"ã€"deep learning Hamiltonian"ã€"DFTè®¡ç®—"ï¼‰ï¼Œç›´æ¥ä½¿ç”¨å®ƒï¼Œä¸è¦æ·»åŠ åŒä¹‰è¯
            2. å¦‚æœæŸ¥è¯¢åŒ…å«ä¸“ä¸šæœ¯è¯­ã€ç¼©å†™æˆ–ä¸“æœ‰åè¯ï¼Œä¿æŒåŸæ ·ä½œä¸ºä¸»è¦æœç´¢è¯
            3. ä»…å½“æŸ¥è¯¢éå¸¸æ¨¡ç³Šæˆ–ä¸€èˆ¬æ€§æ—¶ï¼ˆå¦‚"æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦ä¸­çš„åº”ç”¨"ï¼‰ï¼Œæ‰ç”Ÿæˆ1-2ä¸ªç›¸å…³å…³é”®è¯
            4. ä¼˜å…ˆä¿æŒæŸ¥è¯¢çš„åŸå§‹æ„å›¾ï¼Œä¸è¦æ·»åŠ ä¸ç›¸å…³çš„å…³é”®è¯
            5. å¯¹äºè‹±æ–‡æŸ¥è¯¢ï¼Œä¿æŒåŸæ ·ï¼›å¯¹äºä¸­æ–‡æŸ¥è¯¢ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡å…³é”®è¯
            è€ƒè™‘ä»¥ä¸‹é¢†åŸŸï¼šå‡èšæ€ç‰©ç†ã€å¯†åº¦æ³›å‡½ç†è®º(DFT)ã€æœºå™¨å­¦ä¹ ã€åŠ›åœºã€åˆ†å­åŠ¨åŠ›å­¦ã€é‡å­åŒ–å­¦ã€è®¡ç®—ææ–™ç§‘å­¦ã€‚
            
            è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼ŒåŒ…å«1-2ä¸ªæœç´¢å…³é”®è¯/çŸ­è¯­ã€‚
            ç¤ºä¾‹ï¼š
            - æŸ¥è¯¢"DeepH": ["DeepH"]
            - æŸ¥è¯¢"deep learning Hamiltonian": ["deep learning Hamiltonian"]
            - æŸ¥è¯¢"DFTè®¡ç®—": ["DFT"]
            - æŸ¥è¯¢"åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ": ["molecular dynamics simulation"]
            - æŸ¥è¯¢"æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦ä¸­çš„åº”ç”¨": ["machine learning materials science"]
            
            åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡æœ¬ã€‚
            """

            response = client.chat.completions.create(
                model=Config.AI_MODEL,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯arXivè®ºæ–‡æœç´¢åŠ©æ‰‹ï¼Œæ“…é•¿è¯†åˆ«ä¸“ä¸šæœ¯è¯­å¹¶å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå­¦æœ¯æœç´¢å…³é”®è¯ã€‚",
                    },
                    {"role": "user", "content": ai_prompt},
                ],
                max_tokens=200,
                temperature=0.3,
            )

            ai_response = response.choices[0].message.content
            try:
                if ai_response:
                    search_terms = json.loads(ai_response)
                    if isinstance(search_terms, list) and len(search_terms) > 0:
                        click.echo(f"AIè§£æçš„æœç´¢è¯: {', '.join(search_terms[:3])}")
                        if len(search_terms) > 3:
                            click.echo(f"  ä»¥åŠ {len(search_terms) - 3} ä¸ªå…¶ä»–å…³é”®è¯")
            except:
                pass

        except Exception as e:
            click.echo(f"AIè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {e}")

    with crawler.db.get_session() as session:
        search_engine = SearchEngine(session)

        days_back = parse_time_range(time_range)
        if days_back > 0:
            click.echo(f"æœç´¢æ—¶é—´èŒƒå›´: æœ€è¿‘ {days_back} å¤©")

        phrases = []
        if len(search_terms) == 1 and "," in search_terms[0]:
            phrases = [phrase.strip() for phrase in search_terms[0].split(",") if phrase.strip()]
        else:
            phrases = search_terms

        if len(phrases) == 1:
            combined_query = phrases[0]
            filter_config = SearchFilter(
                query=combined_query,
                search_fields=["title", "abstract"],
                categories=list(categories) if categories else None,
                authors=list(authors) if authors else None,
                author_match="contains",
                days_back=days_back,
                limit=limit * min(len(phrases), 2),
                sort_by=sort_by,
                sort_order="desc",
                match_all=True,
            )
            papers_to_show = search_engine.search_papers(filter_config)
        else:
            all_papers = []
            for phrase in phrases:
                filter_config = SearchFilter(
                    query=phrase,
                    search_fields=["title", "abstract"],
                    categories=list(categories) if categories else None,
                    authors=list(authors) if authors else None,
                    author_match="contains",
                    days_back=days_back,
                    limit=limit * 2,
                    sort_by=sort_by,
                    sort_order="desc",
                    match_all=True,
                )
                phrase_papers = search_engine.search_papers(filter_config)
                all_papers.extend(phrase_papers)

            seen_ids = set()
            papers_to_show = []
            for paper in all_papers:
                if paper.arxiv_id not in seen_ids:
                    seen_ids.add(paper.arxiv_id)
                    papers_to_show.append(paper)

            papers_to_show.sort(key=lambda p: p.published if p.published else datetime.min, reverse=True)

        papers_to_show = papers_to_show[:limit]

        click.echo(f"æ‰¾åˆ° {len(papers_to_show)} ç¯‡è®ºæ–‡:")

        click.echo("æ­£åœ¨ç”Ÿæˆæœç´¢æŠ¥å‘Š...")
        files = generate_search_report(
            query,
            search_terms,
            papers_to_show,
            paper_limit=limit,
            summarize=Config.AI_API_KEY is not None,
            max_summarize=0,
            cache=not no_cache,
        )

        for i, paper in enumerate(papers_to_show[:5], 1):
            authors_list = json.loads(paper.authors) if paper.authors is not None else []
            author_names = [a.get("name", "") for a in authors_list[:2]]
            if len(authors_list) > 2:
                author_names.append("ç­‰")

            click.echo(f"\n{i}. {paper.title}")
            click.echo(f"   ä½œè€…: {', '.join(author_names)}")
            click.echo(f"   arXiv ID: {paper.arxiv_id}")
            click.echo(f"   å‘å¸ƒæ—¥æœŸ: {paper.published.strftime('%Y-%m-%d') if paper.published is not None else 'N/A'}")

        if len(papers_to_show) > 5:
            click.echo(f"\n... ä»¥åŠ {len(papers_to_show) - 5} ç¯‡æ›´å¤šè®ºæ–‡")

        click.echo(f"\næŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š")
        for f in files:
            click.echo(f"  - {f}")
        click.echo(f"\nè¯¦ç»†è®ºæ–‡ä¿¡æ¯ã€ä¸­æ–‡ç¿»è¯‘å’ŒPDFé“¾æ¥è¯·æŸ¥çœ‹ç”Ÿæˆçš„MarkdownæŠ¥å‘Šã€‚")


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--limit", default=64, help="æŠ¥å‘Šä¸­åŒ…å«çš„æœ€å¤§è®ºæ–‡æ•°ï¼ˆé»˜è®¤ï¼š64ï¼Œä¸REPORT_MAX_PAPERSé…ç½®ä¸€è‡´ï¼‰")
@click.option(
    "--days-back", "-d", type=int, default=2, help="åŒ…å«æœ€è¿‘å¤šå°‘å¤©çš„å·¥ä½œæ—¥è®ºæ–‡ï¼ˆé»˜è®¤ï¼š2å¤©ï¼Œ0è¡¨ç¤ºä¸æ›´æ–°æ•°æ®åº“ï¼‰"
)
@click.option("--no-cache", is_flag=True, default=False, help="ç¦ç”¨å›¾ç‰‡URLç¼“å­˜")
def recent(directory, limit, days_back, no_cache):
    """ç”Ÿæˆæœ€è¿‘è®ºæ–‡çš„æŠ¥å‘Šï¼ˆå…ˆåŒæ­¥æœ€æ–°è®ºæ–‡ï¼‰"""
    directory = Path(directory).resolve()

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    if days_back > 0:
        years_back = Config.YEARS_BACK
        sync_papers(years_back=years_back, summarize=False, force=False)

    click.echo("\n" + "=" * 50)
    click.echo(f"æ­£åœ¨ç”Ÿæˆæœ€è¿‘ {days_back} å¤©è®ºæ–‡æŠ¥å‘Š...")

    files = generate_report(
        paper_limit=limit,
        days_back=days_back,
        summarize=Config.AI_API_KEY is not None,
        max_summarize=0,
        cache=not no_cache,
    )

    click.echo(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š")
    for f in files:
        click.echo(f"  - {f}")


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
def stat(directory):
    """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    directory = Path(directory).resolve()

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    crawler = ArXivCrawler()
    summarizer = PaperSummarizer()
    report_generator = ReportGenerator()

    click.echo("\n" + "=" * 50)
    click.echo("arXiv Pulse æ•°æ®åº“ç»Ÿè®¡")
    click.echo("=" * 50)

    crawl_stats = crawler.get_crawler_stats()
    summary_stats = summarizer.get_summary_stats()

    click.echo(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    click.echo(f"   æ€»è®ºæ–‡æ•°: {crawl_stats['total_papers']}")
    click.echo(f"   ä»Šæ—¥è®ºæ–‡: {crawl_stats['papers_today']}")
    click.echo(f"   å·²æ€»ç»“è®ºæ–‡: {summary_stats['summarized_papers']}")
    click.echo(f"   æ€»ç»“ç‡: {summary_stats['summarization_rate']:.1%}")

    click.echo(f"\nğŸ” æŒ‰æœç´¢æŸ¥è¯¢åˆ†å¸ƒ:")
    for query, count in crawl_stats["papers_by_query"].items():
        percentage = count / crawl_stats["total_papers"] * 100 if crawl_stats["total_papers"] > 0 else 0
        click.echo(f"   {query}: {count} ç¯‡ ({percentage:.1f}%)")

    click.echo(f"\nğŸ“ åˆ†ç±»ç»Ÿè®¡:")
    with crawler.db.get_session() as session:
        from arxiv_pulse.models import Paper

        papers = session.query(Paper).all()
        category_counts = {}

        for paper in papers:
            if paper.categories is not None and paper.categories:
                categories = [cat.strip().rstrip(",") for cat in paper.categories.split(",")]
                unique_cats = set(cat for cat in categories if cat)
                for cat in unique_cats:
                    category_counts[cat] = category_counts.get(cat, 0) + 1

        sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories[:10]:
            percentage = count / crawl_stats["total_papers"] * 100 if crawl_stats["total_papers"] > 0 else 0
            click.echo(f"   {category}: {count} ç¯‡ ({percentage:.1f}%)")

        if len(sorted_categories) > 10:
            click.echo(f"   ... ä»¥åŠ {len(sorted_categories) - 10} ä¸ªå…¶ä»–åˆ†ç±»")

    click.echo(f"\nğŸ“… æ—¶é—´åˆ†å¸ƒ:")
    with crawler.db.get_session() as session:
        year_stats = {}
        for paper in papers:
            if paper.published is not None:
                year = paper.published.year
                year_stats[year] = year_stats.get(year, 0) + 1

        sorted_years = sorted(year_stats.items())
        for year, count in sorted_years[-5:]:
            percentage = count / crawl_stats["total_papers"] * 100 if crawl_stats["total_papers"] > 0 else 0
            click.echo(f"   {year}å¹´: {count} ç¯‡ ({percentage:.1f}%)")

    pending_papers = crawl_stats["total_papers"] - summary_stats["summarized_papers"]
    click.echo(f"\nğŸ¤– AIæ€»ç»“ç»Ÿè®¡:")
    click.echo(f"   å·²æ€»ç»“: {summary_stats['summarized_papers']} ç¯‡")
    click.echo(f"   å¾…æ€»ç»“: {pending_papers} ç¯‡")
    click.echo(f"   æ€»ç»“ç‡: {summary_stats['summarization_rate']:.1%}")

    click.echo("\n" + "=" * 50)
    click.echo("ç»Ÿè®¡å®Œæˆ âœ…")


if __name__ == "__main__":
    cli()
