#!/usr/bin/env python3
"""
arXiv Pulse - ç®€åŒ–ç‰ˆå‘½ä»¤è¡Œç•Œé¢
æ ¸å¿ƒåŠŸèƒ½ï¼šåˆå§‹åŒ–ã€æ›´æ–°åŒæ­¥ã€æ™ºèƒ½æœç´¢ã€æœ€è¿‘è®ºæ–‡æŠ¥å‘Š
"""

import os
import sys
from pathlib import Path
import click
from dotenv import load_dotenv
import json
from datetime import datetime, timedelta, timezone
import questionary
import wcwidth

from arxiv_pulse.config import Config
from arxiv_pulse.arxiv_crawler import ArXivCrawler
from arxiv_pulse.summarizer import PaperSummarizer
from arxiv_pulse.report_generator import ReportGenerator
from arxiv_pulse.output_manager import output
from arxiv_pulse.search_engine import SearchEngine, SearchFilter
from arxiv_pulse.__version__ import __version__


# arXivç ”ç©¶é¢†åŸŸå®šä¹‰ï¼ˆç”¨äºäº¤äº’å¼é…ç½®å’Œæ¨ªå¹…ç”Ÿæˆï¼‰
RESEARCH_FIELDS = {
    # ç‰©ç†å­¦é¢†åŸŸ
    "condensed_matter": {
        "name": "å‡èšæ€ç‰©ç†",
        "query": "condensed matter physics AND cat:cond-mat.*",
        "description": "åŒ…æ‹¬è¶…å¯¼ã€å¼ºå…³è”ç”µå­ã€ä»‹è§‚ç³»ç»Ÿã€ææ–™ç§‘å­¦ç­‰",
        "keywords": ["condensed matter physics", "cond-mat"],
    },
    "astro_physics": {
        "name": "å¤©ä½“ç‰©ç†",
        "query": "cat:astro-ph.*",
        "description": "å¤©ä½“ç‰©ç†å­¦ã€å®‡å®™å­¦ã€å¤©ä½“è§‚æµ‹ç­‰",
        "keywords": ["astro-ph"],
    },
    "high_energy_physics": {
        "name": "é«˜èƒ½ç‰©ç†ï¼ˆç²’å­ç‰©ç†ï¼‰",
        "query": "cat:hep-ph.* OR cat:hep-ex.* OR cat:hep-th.* OR cat:hep-lat.*",
        "description": "ç²’å­ç‰©ç†ã€é«˜èƒ½ç‰©ç†ç†è®ºä¸å®éªŒ",
        "keywords": ["hep-ph", "hep-ex", "hep-th", "hep-lat"],
    },
    "nuclear_physics": {
        "name": "æ ¸ç‰©ç†",
        "query": "cat:nucl-th.* OR cat:nucl-ex.*",
        "description": "æ ¸ç‰©ç†ç†è®ºä¸å®éªŒ",
        "keywords": ["nucl-th", "nucl-ex"],
    },
    "general_relativity": {
        "name": "å¹¿ä¹‰ç›¸å¯¹è®ºä¸å®‡å®™å­¦",
        "query": "cat:gr-qc.*",
        "description": "å¼•åŠ›ç†è®ºã€å®‡å®™å­¦ã€é»‘æ´ç‰©ç†",
        "keywords": ["gr-qc"],
    },
    "quantum_physics": {
        "name": "é‡å­ç‰©ç†",
        "query": "cat:quant-ph.*",
        "description": "é‡å­ä¿¡æ¯ã€é‡å­è®¡ç®—ã€é‡å­åŸºç¡€",
        "keywords": ["quant-ph"],
    },
    "computational_physics": {
        "name": "è®¡ç®—ç‰©ç†",
        "query": "cat:physics.comp-ph",
        "description": "æ•°å€¼è®¡ç®—æ–¹æ³•åœ¨ç‰©ç†ä¸­çš„åº”ç”¨",
        "keywords": ["physics.comp-ph"],
    },
    "chemical_physics": {
        "name": "åŒ–å­¦ç‰©ç†",
        "query": "cat:physics.chem-ph",
        "description": "åŒ–å­¦è¿‡ç¨‹çš„ç‰©ç†åŸºç¡€",
        "keywords": ["physics.chem-ph"],
    },
    "physics_other": {
        "name": "ç‰©ç†å­¦ï¼ˆå…¶ä»–ï¼‰",
        "query": "cat:physics:* NOT cat:physics.comp-ph NOT cat:physics.chem-ph",
        "description": "å…¶ä»–ç‰©ç†å­¦é¢†åŸŸ",
        "keywords": ["physics:"],
    },
    "nonlinear_science": {
        "name": "éçº¿æ€§ç§‘å­¦",
        "query": "cat:nlin.*",
        "description": "éçº¿æ€§åŠ¨åŠ›å­¦ã€å¤æ‚ç³»ç»Ÿã€æ··æ²Œç†è®º",
        "keywords": ["nlin"],
    },
    "mathematical_physics": {
        "name": "æ•°å­¦ç‰©ç†",
        "query": "cat:math-ph.*",
        "description": "ç‰©ç†é—®é¢˜çš„æ•°å­¦æ–¹æ³•",
        "keywords": ["math-ph"],
    },
    # è®¡ç®—ææ–™ç§‘å­¦ä¸“ä¸šé¢†åŸŸ
    "dft": {
        "name": "å¯†åº¦æ³›å‡½ç†è®º (DFT)",
        "query": '(ti:"density functional" OR abs:"density functional") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci OR cat:physics.chem-ph)',
        "description": "ç¬¬ä¸€æ€§åŸç†è®¡ç®—ã€ææ–™è®¾è®¡",
        "keywords": ["density functional"],
    },
    "first_principles": {
        "name": "ç¬¬ä¸€æ€§åŸç†è®¡ç®—",
        "query": '(ti:"first principles" OR abs:"first principles" OR ti:"ab initio" OR abs:"ab initio") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci)',
        "description": "ä»å¤´è®¡ç®—ã€é‡å­åŒ–å­¦æ–¹æ³•",
        "keywords": ["first principles", "ab initio"],
    },
    "quantum_chemistry": {
        "name": "é‡å­åŒ–å­¦",
        "query": '(ti:"quantum chemistry" OR abs:"quantum chemistry") AND (cat:physics.chem-ph OR cat:physics.comp-ph)',
        "description": "é‡å­åŒ–å­¦æ–¹æ³•ä¸è®¡ç®—",
        "keywords": ["quantum chemistry"],
    },
    "force_fields": {
        "name": "åŠ›åœºä¸åˆ†å­åŠ¨åŠ›å­¦",
        "query": '(ti:"force field" OR abs:"force field") AND (cat:physics.comp-ph OR cat:cond-mat.soft OR cat:physics.chem-ph)',
        "description": "åŠ›åœºå¼€å‘ã€åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ",
        "keywords": ["force field"],
    },
    "molecular_dynamics": {
        "name": "åˆ†å­åŠ¨åŠ›å­¦",
        "query": '(ti:"molecular dynamics" OR abs:"molecular dynamics") AND (cat:physics.comp-ph OR cat:cond-mat.soft OR cat:physics.chem-ph)',
        "description": "åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹ŸæŠ€æœ¯",
        "keywords": ["molecular dynamics"],
    },
    "computational_materials": {
        "name": "è®¡ç®—ææ–™ç§‘å­¦",
        "query": 'cat:cond-mat.mtrl-sci AND (ti:"computational" OR abs:"computational" OR ti:"simulation" OR abs:"simulation")',
        "description": "ææ–™è®¡ç®—ä¸æ¨¡æ‹Ÿ",
        "keywords": ["computational materials", "materials science"],
    },
    # æ•°å­¦é¢†åŸŸ
    "mathematics": {
        "name": "æ•°å­¦",
        "query": "cat:math.* AND NOT cat:math-ph.*",
        "description": "çº¯æ•°å­¦ä¸åº”ç”¨æ•°å­¦",
        "keywords": ["cat:math."],
    },
    "numerical_analysis": {
        "name": "æ•°å€¼åˆ†æ",
        "query": "cat:math.NA",
        "description": "æ•°å€¼è®¡ç®—æ–¹æ³•ä¸ç®—æ³•",
        "keywords": ["math.NA"],
    },
    "optimization_control": {
        "name": "ä¼˜åŒ–ä¸æ§åˆ¶",
        "query": "cat:math.OC",
        "description": "æ•°å­¦ä¼˜åŒ–ã€æœ€ä¼˜æ§åˆ¶ç†è®º",
        "keywords": ["math.OC"],
    },
    "statistics_math": {
        "name": "ç»Ÿè®¡å­¦ï¼ˆæ•°å­¦ï¼‰",
        "query": "cat:math.ST",
        "description": "æ•°ç†ç»Ÿè®¡ç†è®º",
        "keywords": ["math.ST"],
    },
    # è®¡ç®—æœºç§‘å­¦é¢†åŸŸ
    "machine_learning": {
        "name": "æœºå™¨å­¦ä¹ ",
        "query": '(ti:"machine learning" OR abs:"machine learning") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci OR cat:physics.chem-ph OR cat:cs.* OR cat:stat.*)',
        "description": "æœºå™¨å­¦ä¹ åœ¨ç‰©ç†å’Œææ–™ç§‘å­¦ä¸­çš„åº”ç”¨",
        "keywords": ["machine learning"],
    },
    "artificial_intelligence": {
        "name": "äººå·¥æ™ºèƒ½",
        "query": "cat:cs.AI OR cat:cs.LG OR cat:cs.NE",
        "description": "äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€ç¥ç»ç½‘ç»œ",
        "keywords": ["cs.AI", "cs.LG", "cs.NE"],
    },
    "computer_vision": {
        "name": "è®¡ç®—æœºè§†è§‰",
        "query": "cat:cs.CV",
        "description": "å›¾åƒå¤„ç†ã€è®¡ç®—æœºè§†è§‰",
        "keywords": ["cs.CV"],
    },
    "natural_language": {
        "name": "è‡ªç„¶è¯­è¨€å¤„ç†",
        "query": "cat:cs.CL",
        "description": "è®¡ç®—è¯­è¨€å­¦ã€è‡ªç„¶è¯­è¨€å¤„ç†",
        "keywords": ["cs.CL"],
    },
    "computer_science_other": {
        "name": "è®¡ç®—æœºç§‘å­¦ï¼ˆå…¶ä»–ï¼‰",
        "query": "cat:cs.* NOT cat:cs.AI NOT cat:cs.LG NOT cat:cs.NE NOT cat:cs.CV NOT cat:cs.CL",
        "description": "å…¶ä»–è®¡ç®—æœºç§‘å­¦é¢†åŸŸ",
        "keywords": ["cat:cs."],
    },
    # ç»Ÿè®¡å­¦é¢†åŸŸ
    "statistics": {
        "name": "ç»Ÿè®¡å­¦",
        "query": "cat:stat.*",
        "description": "ç»Ÿè®¡å­¦ç†è®ºä¸åº”ç”¨",
        "keywords": ["cat:stat."],
    },
    "statistical_learning": {
        "name": "ç»Ÿè®¡å­¦ä¹ ",
        "query": "cat:stat.ML",
        "description": "ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸åº”ç”¨",
        "keywords": ["stat.ML"],
    },
    # è·¨å­¦ç§‘é¢†åŸŸ
    "quantitative_biology": {
        "name": "å®šé‡ç”Ÿç‰©å­¦",
        "query": "cat:q-bio.*",
        "description": "ç”Ÿç‰©ä¿¡æ¯å­¦ã€ç³»ç»Ÿç”Ÿç‰©å­¦ã€å®šé‡ç”Ÿç‰©æ–¹æ³•",
        "keywords": ["q-bio"],
    },
    "quantitative_finance": {
        "name": "å®šé‡é‡‘è",
        "query": "cat:q-fin.*",
        "description": "é‡‘èæ•°å­¦ã€é‡‘èå·¥ç¨‹ã€è®¡é‡é‡‘è",
        "keywords": ["q-fin"],
    },
    "electrical_engineering": {
        "name": "ç”µå­å·¥ç¨‹ä¸ç³»ç»Ÿç§‘å­¦",
        "query": "cat:eess.*",
        "description": "ä¿¡å·å¤„ç†ã€æ§åˆ¶ç³»ç»Ÿã€ç”µå­å·¥ç¨‹",
        "keywords": ["eess"],
    },
    "economics": {
        "name": "ç»æµå­¦",
        "query": "cat:econ.*",
        "description": "ç»æµå­¦ç†è®ºã€è®¡é‡ç»æµå­¦",
        "keywords": ["econ"],
    },
}


def setup_environment(directory: Path):
    """è®¾ç½®ç¯å¢ƒå¹¶éªŒè¯ç»™å®šç›®å½•çš„é…ç½®"""
    original_cwd = os.getcwd()
    try:
        os.chdir(directory)

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs("data", exist_ok=True)
        os.makedirs("reports", exist_ok=True)
        os.makedirs("logs", exist_ok=True)

        # åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        env_file = directory / ".env"
        if env_file.exists():
            load_dotenv(env_file)
        else:
            output.warn(f"åœ¨ {directory} ä¸­æœªæ‰¾åˆ° .env æ–‡ä»¶ã€‚ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")

        # å°† DATABASE_URL è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆå¦‚æœæ˜¯ç›¸å¯¹ SQLite è·¯å¾„ï¼‰
        db_url = os.getenv("DATABASE_URL", "sqlite:///data/arxiv_papers.db")
        if db_url.startswith("sqlite:///") and not db_url.startswith("sqlite:////"):
            # ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            db_path = db_url.replace("sqlite:///", "")
            abs_db_path = os.path.abspath(db_path)
            os.environ["DATABASE_URL"] = f"sqlite:///{abs_db_path}"
            output.debug(f"Converted DATABASE_URL to absolute path: {os.environ['DATABASE_URL']}")

        # ç›´æ¥æ›´æ–° Config ç±»å˜é‡
        Config.DATABASE_URL = os.environ["DATABASE_URL"]
        # åŸºäºæ–°çš„ DATABASE_URL æ›´æ–° DATA_DIR
        Config.DATA_DIR = os.path.dirname(Config.DATABASE_URL.replace("sqlite:///", ""))
        # å¦‚æœç›¸å¯¹ï¼Œå°† REPORT_DIR è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        report_dir = os.getenv("REPORT_DIR", "reports")
        if not os.path.isabs(report_dir):
            Config.REPORT_DIR = os.path.abspath(report_dir)
            output.debug(f"Converted REPORT_DIR to absolute path: {Config.REPORT_DIR}")

        # ä»ç¯å¢ƒå˜é‡æ›´æ–°å…¶ä»– Config å˜é‡
        Config.AI_API_KEY = os.getenv("AI_API_KEY")
        Config.AI_MODEL = os.getenv("AI_MODEL", "DeepSeek-V3.2-Thinking")
        Config.AI_BASE_URL = os.getenv("AI_BASE_URL", "https://llmapi.paratera.com")
        Config.SUMMARY_MAX_TOKENS = int(os.getenv("SUMMARY_MAX_TOKENS", "2000"))
        Config.SUMMARY_SENTENCES_LIMIT = int(os.getenv("SUMMARY_SENTENCES_LIMIT", "3"))
        Config.TOKEN_PRICE_PER_MILLION = float(os.getenv("TOKEN_PRICE_PER_MILLION", "3.0"))
        Config.MAX_RESULTS_INITIAL = int(os.getenv("MAX_RESULTS_INITIAL", "100"))
        Config.MAX_RESULTS_DAILY = int(os.getenv("MAX_RESULTS_DAILY", "20"))
        Config.YEARS_BACK = int(os.getenv("YEARS_BACK", "3"))
        Config.IMPORTANT_PAPERS_FILE = os.getenv("IMPORTANT_PAPERS_FILE", "important_papers.txt")
        Config.REPORT_MAX_PAPERS = int(os.getenv("REPORT_MAX_PAPERS", "50"))

        # æ›´æ–° SEARCH_QUERIES
        search_queries_raw = os.getenv(
            "SEARCH_QUERIES",
            "condensed matter physics; density functional theory; machine learning; force fields; first principles calculation; molecular dynamics; quantum chemistry; computational materials science",
        )
        Config.SEARCH_QUERIES_RAW = search_queries_raw
        Config.SEARCH_QUERIES = [q.strip() for q in search_queries_raw.split(";") if q.strip()]

        # éªŒè¯é…ç½®
        try:
            Config.validate()
            output.info("é…ç½®éªŒè¯é€šè¿‡")
        except Exception as e:
            output.error(f"é…ç½®é”™è¯¯: {e}")
            return False

        return True
    finally:
        os.chdir(original_cwd)


def print_banner():
    """æ‰“å°åº”ç”¨æ¨ªå¹…"""
    print_banner_custom(["å‡èšæ€ç‰©ç†", "å¯†åº¦æ³›å‡½ç†è®º", "æœºå™¨å­¦ä¹ ", "åŠ›åœº"])


def generate_banner_title(env_file):
    """æ ¹æ®é…ç½®æ–‡ä»¶ç”Ÿæˆæ¨ªå¹…æ ‡é¢˜"""
    try:
        # è¯»å– .env æ–‡ä»¶ï¼Œè§£æ SEARCH_QUERIES
        import re
        from pathlib import Path

        env_path = Path(env_file) if isinstance(env_file, str) else env_file
        if not env_path.exists():
            return ["å‡èšæ€ç‰©ç†", "å¯†åº¦æ³›å‡½ç†è®º", "æœºå™¨å­¦ä¹ ", "åŠ›åœº"]

        with open(env_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æå– SEARCH_QUERIES
        queries_match = re.search(r"SEARCH_QUERIES=(.*?)(?:\n#|\n$)", content, re.DOTALL | re.MULTILINE)
        if not queries_match:
            return ["å‡èšæ€ç‰©ç†", "å¯†åº¦æ³›å‡½ç†è®º", "æœºå™¨å­¦ä¹ ", "åŠ›åœº"]

        queries = queries_match.group(1).strip()

        # æ ¹æ®æŸ¥è¯¢ç¡®å®šé¢†åŸŸ
        fields = []

        # ä½¿ç”¨ RESEARCH_FIELDS è¿›è¡Œæ™ºèƒ½åŒ¹é…
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¯èƒ½çš„åŒ¹é…
        matched_fields = []

        for field_id, field_info in RESEARCH_FIELDS.items():
            field_name = field_info["name"]
            keywords = field_info.get("keywords", [])

            # æ£€æŸ¥æ¯ä¸ªå…³é”®è¯æ˜¯å¦å‡ºç°åœ¨æŸ¥è¯¢ä¸­
            for keyword in keywords:
                # å¯¹å…³é”®è¯è¿›è¡Œè½¬ä¹‰ï¼Œä»¥ä¾¿åœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­ä½¿ç”¨
                # ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                pattern = re.escape(keyword)
                if re.search(pattern, queries, re.IGNORECASE):
                    # è®°å½•åŒ¹é…çš„å­—æ®µå’ŒåŒ¹é…çš„å…³é”®è¯æ•°é‡ï¼ˆç”¨äºæ’åºï¼‰
                    matched_fields.append(
                        {
                            "id": field_id,
                            "name": field_name,
                            "match_count": 1,  # ç®€å•è®¡æ•°ï¼Œå¯ä»¥æ›´å¤æ‚
                        }
                    )
                    break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±è¶³å¤Ÿ

        # æ ¹æ®åŒ¹é…æƒ…å†µé€‰æ‹©è¦æ˜¾ç¤ºçš„å­—æ®µ
        if matched_fields:
            # å»é‡ï¼ˆæŒ‰å­—æ®µåï¼‰
            seen_names = set()
            for match in matched_fields:
                if match["name"] not in seen_names:
                    fields.append(match["name"])
                    seen_names.add(match["name"])

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤
        if not fields:
            # å°è¯•åŸºäºæŸ¥è¯¢å†…å®¹è¿›è¡Œæ›´å®½æ¾çš„åŒ¹é…
            queries_lower = queries.lower()

            # å¸¸è§çš„arXivåˆ†ç±»æ£€æµ‹
            if "cond-mat" in queries_lower or "condensed matter" in queries_lower:
                fields.append("å‡èšæ€ç‰©ç†")
            if "astro-ph" in queries_lower:
                fields.append("å¤©ä½“ç‰©ç†")
            if "hep-" in queries_lower:
                fields.append("é«˜èƒ½ç‰©ç†")
            if "quant-ph" in queries_lower:
                fields.append("é‡å­ç‰©ç†")
            if "physics.comp-ph" in queries_lower:
                fields.append("è®¡ç®—ç‰©ç†")
            if "math." in queries_lower:
                fields.append("æ•°å­¦")
            if "cs." in queries_lower:
                fields.append("è®¡ç®—æœºç§‘å­¦")
            if "stat." in queries_lower:
                fields.append("ç»Ÿè®¡å­¦")

            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤
            if not fields:
                return ["å‡èšæ€ç‰©ç†", "å¯†åº¦æ³›å‡½ç†è®º", "æœºå™¨å­¦ä¹ ", "åŠ›åœº"]

        # é™åˆ¶æœ€å¤šæ˜¾ç¤º4ä¸ªé¢†åŸŸ
        return fields[:4]

    except Exception as e:
        # å‡ºé”™æ—¶è¿”å›é»˜è®¤
        return ["å‡èšæ€ç‰©ç†", "å¯†åº¦æ³›å‡½ç†è®º", "æœºå™¨å­¦ä¹ ", "åŠ›åœº"]


def print_banner_custom(fields):
    """æ‰“å°è‡ªå®šä¹‰å­—æ®µçš„åº”ç”¨æ¨ªå¹…"""
    # åˆ›å»ºå­—æ®µå­—ç¬¦ä¸²
    if len(fields) == 0:
        field_str = "å‡èšæ€ç‰©ç† â€¢ å¯†åº¦æ³›å‡½ç†è®º â€¢ æœºå™¨å­¦ä¹  â€¢ åŠ›åœº"
    elif len(fields) == 1:
        field_str = fields[0]
    elif len(fields) == 2:
        field_str = f"{fields[0]} â€¢ {fields[1]}"
    elif len(fields) == 3:
        field_str = f"{fields[0]} â€¢ {fields[1]} â€¢ {fields[2]}"
    else:
        field_str = f"{fields[0]} â€¢ {fields[1]} â€¢ {fields[2]} â€¢ {fields[3]}"

    # æ¨ªå¹…å°ºå¯¸
    banner_width = 55
    content_width = 53

    # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—å­—ç¬¦ä¸²æ˜¾ç¤ºå®½åº¦
    def display_width(text):
        return wcwidth.wcswidth(text)

    # è¾…åŠ©å‡½æ•°ï¼šæˆªæ–­å­—ç¬¦ä¸²åˆ°æŒ‡å®šæ˜¾ç¤ºå®½åº¦ï¼Œæ·»åŠ çœç•¥å·
    def truncate_to_width(text, max_width):
        if display_width(text) <= max_width:
            return text
        # é€æ­¥å‡å°‘å­—ç¬¦ç›´åˆ°å®½åº¦åˆé€‚
        result = ""
        for char in text:
            if display_width(result + char) > max_width - 3:  # ä¸º"..."ç•™å‡ºç©ºé—´
                break
            result += char
        return result + "..." if result else "..."  # è‡³å°‘è¿”å›çœç•¥å·

    # åˆ›å»ºæ¨ªå¹…è¾¹æ¡†
    border_top = "â•”" + "â•" * (banner_width - 2) + "â•—"
    border_bottom = "â•š" + "â•" * (banner_width - 2) + "â•"

    # ç¬¬ä¸€è¡Œæ ‡é¢˜
    title = "arXiv Pulse - æ–‡çŒ®è¿½è¸ªç³»ç»Ÿ"
    title_width = display_width(title)
    # è®¡ç®—å·¦å³å¡«å……
    left_padding = (content_width - title_width) // 2
    right_padding = content_width - title_width - left_padding
    title_line = "â•‘" + " " * left_padding + title + " " * right_padding + "â•‘"

    # ç¬¬äºŒè¡Œå­—æ®µ
    # æœ€å¤§å­—æ®µæ˜¾ç¤ºå®½åº¦ï¼ˆç•™å‡ºè¾¹è·ï¼‰
    max_field_width = content_width - 4
    # æˆªæ–­å­—æ®µå­—ç¬¦ä¸²å¦‚æœå¤ªé•¿
    field_str = truncate_to_width(field_str, max_field_width)
    field_width = display_width(field_str)

    # è®¡ç®—å­—æ®µè¡Œçš„å·¦å³å¡«å……
    left_padding = (content_width - field_width) // 2
    right_padding = content_width - field_width - left_padding
    field_line = "â•‘" + " " * left_padding + field_str + " " * right_padding + "â•‘"

    banner = f"\n{border_top}\n{title_line}\n{field_line}\n{border_bottom}\n"
    click.echo(banner)


def sync_papers(years_back=1, summarize=False, force=False):
    """åŒæ­¥è®ºæ–‡ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰

    Args:
        years_back: å›æº¯çš„å¹´æ•°
        summarize: æ˜¯å¦æ€»ç»“æ–°è®ºæ–‡
        force: æ˜¯å¦å¼ºåˆ¶åŒæ­¥ï¼ˆé‡æ–°ä¸‹è½½æ‰€æœ‰è®ºæ–‡ï¼Œå¿½ç•¥é‡å¤æ£€æŸ¥ï¼‰
    """
    crawler = ArXivCrawler()
    summarizer = PaperSummarizer()

    mode_text = "å¼ºåˆ¶åŒæ­¥" if force else "åŒæ­¥ç¼ºå¤±è®ºæ–‡"
    click.echo(f"æ­£åœ¨{mode_text}ï¼ˆå›æº¯ {years_back} å¹´ï¼‰...")
    click.echo("=" * 50)

    # åŒæ­¥æ‰€æœ‰æŸ¥è¯¢
    click.echo("1. æ­£åœ¨åŒæ­¥æœç´¢æŸ¥è¯¢...")
    sync_result = crawler.sync_all_queries(years_back=years_back, force=force)
    result_text = "å¤„ç†äº†" if force else "æ·»åŠ äº†"
    click.echo(f"   ä»æŸ¥è¯¢{result_text} {sync_result['total_new_papers']} ç¯‡è®ºæ–‡")

    # åŒæ­¥é‡è¦è®ºæ–‡
    click.echo("2. æ­£åœ¨åŒæ­¥é‡è¦è®ºæ–‡...")
    important_result = crawler.sync_important_papers()
    click.echo(f"   æ·»åŠ äº† {important_result['added']} ç¯‡é‡è¦è®ºæ–‡")
    if important_result["errors"]:
        click.echo(f"   é”™è¯¯: {len(important_result['errors'])}")

    # æ€»ç»“æ–°è®ºæ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    total_new = sync_result["total_new_papers"] + important_result["added"]
    if summarize and total_new > 0:
        click.echo("3. æ­£åœ¨æ€»ç»“æ–°è®ºæ–‡...")
        summarize_result = summarizer.summarize_pending_papers(limit=min(50, total_new))
        click.echo(f"   å·²æ€»ç»“ {summarize_result['successful']} ç¯‡è®ºæ–‡")
    elif total_new > 0:
        click.echo("3. è·³è¿‡è®ºæ–‡æ€»ç»“")
    else:
        click.echo("3. æ²¡æœ‰æ–°è®ºæ–‡éœ€è¦æ€»ç»“")

    # æ›´æ–°æ•°æ®åº“ç»Ÿè®¡
    crawl_stats = crawler.get_crawler_stats()
    summary_stats = summarizer.get_summary_stats()

    click.echo("\n" + "=" * 50)
    click.echo("åŒæ­¥å®Œæˆï¼")
    click.echo(f"æ€»å…±{result_text}è®ºæ–‡: {total_new}")
    click.echo(f"æ•°æ®åº“ç°æœ‰ {crawl_stats['total_papers']} ç¯‡è®ºæ–‡")
    click.echo(f"å·²æ€»ç»“: {summary_stats['summarized_papers']} ({summary_stats['summarization_rate']:.1%})")

    return {
        "crawler": crawler,
        "summarizer": summarizer,
        "sync_result": sync_result,
        "important_result": important_result,
        "stats": {"crawl_stats": crawl_stats, "summary_stats": summary_stats},
        "force_mode": force,
    }


def get_workday_cutoff(days_back):
    """è®¡ç®—æ’é™¤å‘¨æœ«çš„æˆªæ­¢æ—¥æœŸ"""
    current = datetime.now(timezone.utc).replace(tzinfo=None)
    workdays_counted = 0
    days_to_go_back = 0

    while workdays_counted < days_back:
        days_to_go_back += 1
        # æ£€æŸ¥æ˜¯å¦ä¸ºå·¥ä½œæ—¥ï¼ˆå‘¨ä¸€è‡³å‘¨äº”ï¼‰
        if (current - timedelta(days=days_to_go_back)).weekday() < 5:
            workdays_counted += 1

    return current - timedelta(days=days_to_go_back)


def generate_report(paper_limit=50, days_back=2, summarize=True, max_summarize=10):
    """ç”Ÿæˆæœ€è¿‘è®ºæ–‡çš„æŠ¥å‘Šï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    reporter = ReportGenerator()

    # è®¾ç½®æŠ¥å‘Šé™åˆ¶
    original_limit = Config.REPORT_MAX_PAPERS
    Config.REPORT_MAX_PAPERS = paper_limit

    try:
        # ç”ŸæˆæŠ¥å‘Šæ•°æ®
        with reporter.db.get_session() as session:
            from arxiv_pulse.models import Paper

            # è·å–æœ€è¿‘Nä¸ªå·¥ä½œæ—¥çš„è®ºæ–‡ï¼ˆæ’é™¤å‘¨æœ«ï¼‰
            cutoff = get_workday_cutoff(days_back)
            recent_papers = (
                session.query(Paper)
                .filter(Paper.published >= cutoff)
                .order_by(Paper.published.desc())
                .limit(paper_limit)
                .all()
            )

            # æ€»ç»“æœªæ€»ç»“çš„è®ºæ–‡ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡å¤šAPIè°ƒç”¨ï¼‰
            summarized_count = 0
            summarizer = PaperSummarizer()

            if summarize:
                for paper in recent_papers:
                    if paper.summarized is False and (max_summarize == 0 or summarized_count < max_summarize):
                        if summarizer.summarize_paper(paper):
                            summarized_count += 1
                            # åˆ·æ–°è®ºæ–‡å¯¹è±¡ä»¥è·å–æ›´æ–°åçš„æ€»ç»“æ•°æ®
                            session.refresh(paper)

                if summarized_count > 0:
                    output.info(f"å·²æ€»ç»“ {summarized_count} ç¯‡è®ºæ–‡ç”¨äºæŠ¥å‘Š")
                    # æ˜¾ç¤ºç´¯è®¡tokenä½¿ç”¨æƒ…å†µ
                    summary_stats = summarizer.get_summary_stats()
                    token_usage = summary_stats.get("token_usage", {})
                    if token_usage:
                        output.info(
                            f"ç´¯è®¡Tokenä½¿ç”¨: æç¤º {token_usage.get('total_prompt_tokens', 0)}, "
                            f"å®Œæˆ {token_usage.get('total_completion_tokens', 0)}, "
                            f"æ€»è®¡ {token_usage.get('total_tokens', 0)}"
                        )

            # è®¡ç®—çƒ­é—¨åˆ†ç±»
            category_counts = {}
            for paper in recent_papers:
                if paper.categories is not None:
                    # arXivåˆ†ç±»ä»¥ç©ºæ ¼åˆ†éš”ï¼Œä¾‹å¦‚ "cond-mat.mtrl-sci physics.comp-ph"
                    for cat in paper.categories.split():
                        category_counts[cat] = category_counts.get(cat, 0) + 1

            # å–å‰5ä¸ªçƒ­é—¨åˆ†ç±»
            top_categories = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5])

            # è·å–æ•°æ®åº“æ€»ä½“ç»Ÿè®¡
            crawler = ArXivCrawler()
            crawl_stats = crawler.get_crawler_stats()
            summary_stats = summarizer.get_summary_stats()

            # åˆ›å»ºæŠ¥å‘Šæ•°æ®
            report_data = {
                "stats": {
                    "total_recent": len(recent_papers),
                    "days_back": days_back,
                    "report_type": "recent",
                    "date_generated": datetime.now().isoformat(),
                    "database_stats": {
                        "total_papers": crawl_stats["total_papers"],
                        "summarized_papers": summary_stats["summarized_papers"],
                    },
                    "top_categories": top_categories,
                },
                "papers": recent_papers,
            }

        # ä¿å­˜æŠ¥å‘Š
        files = []

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = reporter.save_markdown_report(report_data)
        if md_file:
            files.append(md_file)

        # ä¿å­˜CSVæŠ¥å‘Š
        csv_file = reporter.save_csv_report(report_data)
        if csv_file:
            files.append(csv_file)

        return files
    finally:
        Config.REPORT_MAX_PAPERS = original_limit


def generate_search_report(query, search_terms, papers, paper_limit=50, summarize=True, max_summarize=10):
    """ç”Ÿæˆæœç´¢ç»“æœçš„æŠ¥å‘Šï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    reporter = ReportGenerator()

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œä¸ç”ŸæˆæŠ¥å‘Š
    if not papers:
        output.info("æœªæ‰¾åˆ°è®ºæ–‡ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        return []

    # è®¾ç½®æŠ¥å‘Šé™åˆ¶
    original_limit = Config.REPORT_MAX_PAPERS
    Config.REPORT_MAX_PAPERS = paper_limit

    try:
        # æ€»ç»“æœªæ€»ç»“çš„è®ºæ–‡ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡å¤šAPIè°ƒç”¨ï¼‰
        summarized_count = 0
        summarizer = PaperSummarizer()

        if summarize:
            # æ”¶é›†éœ€è¦æ€»ç»“çš„è®ºæ–‡ID
            papers_to_summarize = []
            for paper in papers:
                if paper.summarized is False and (max_summarize == 0 or summarized_count < max_summarize):
                    papers_to_summarize.append(paper)
                    summarized_count += 1

            # æ€»ç»“è®ºæ–‡
            for paper in papers_to_summarize:
                summarizer.summarize_paper(paper)

            if summarized_count > 0:
                output.info(f"å·²æ€»ç»“ {summarized_count} ç¯‡è®ºæ–‡ç”¨äºæŠ¥å‘Š")
                # é‡æ–°è·å–è®ºæ–‡æ•°æ®ä»¥ç¡®ä¿åŒ…å«æœ€æ–°æ€»ç»“
                with summarizer.db.get_session() as session:
                    from arxiv_pulse.models import Paper

                    paper_ids = [p.arxiv_id for p in papers]
                    # æŒ‰åŸå§‹é¡ºåºé‡æ–°æŸ¥è¯¢è®ºæ–‡
                    updated_papers = []
                    for paper_id in paper_ids:
                        paper = session.query(Paper).filter_by(arxiv_id=paper_id).first()
                        if paper:
                            updated_papers.append(paper)
                    papers = updated_papers

        # è®¡ç®—çƒ­é—¨åˆ†ç±»
        category_counts = {}
        for paper in papers:
            if paper.categories is not None:
                # arXivåˆ†ç±»ä»¥ç©ºæ ¼åˆ†éš”ï¼Œä¾‹å¦‚ "cond-mat.mtrl-sci physics.comp-ph"
                for cat in paper.categories.split():
                    category_counts[cat] = category_counts.get(cat, 0) + 1

        # å–å‰5ä¸ªçƒ­é—¨åˆ†ç±»
        top_categories = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5])

        # è·å–æ•°æ®åº“æ€»ä½“ç»Ÿè®¡
        crawler = ArXivCrawler()
        summarizer = PaperSummarizer()
        crawl_stats = crawler.get_crawler_stats()
        summary_stats = summarizer.get_summary_stats()

        # åˆ›å»ºæŠ¥å‘Šæ•°æ®
        report_data = {
            "stats": {
                "total_found": len(papers),
                "original_query": query,
                "search_terms": search_terms,
                "report_type": "search",
                "date_generated": datetime.now().isoformat(),
                "database_stats": {
                    "total_papers": crawl_stats["total_papers"],
                    "summarized_papers": summary_stats["summarized_papers"],
                },
                "top_categories": top_categories,
            },
            "papers": papers,
        }

        # ä¿å­˜æŠ¥å‘Š
        files = []

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = reporter.save_markdown_report(report_data)
        if md_file:
            files.append(md_file)

        # ä¿å­˜CSVæŠ¥å‘Š
        csv_file = reporter.save_csv_report(report_data)
        if csv_file:
            files.append(csv_file)

        return files
    finally:
        Config.REPORT_MAX_PAPERS = original_limit


@click.group(context_settings={"help_option_names": ["-h", "--help"]})
@click.version_option(version=__version__, prog_name="arXiv Pulse")
def cli():
    """arXiv Pulse: æ™ºèƒ½arXivæ–‡çŒ®è¿½è¸ªå’Œåˆ†æç³»ç»Ÿ"""
    pass


def interactive_configuration():
    """äº¤äº’å¼é…ç½® arXiv Pulse"""
    config = {}
    import openai

    click.echo("\n" + "=" * 60)
    click.echo("arXiv Pulse äº¤äº’å¼é…ç½®å‘å¯¼")
    click.echo("=" * 60)

    # 1. AI API é…ç½®
    click.echo("\nğŸ”§ AI API é…ç½®")
    click.echo("-" * 40)

    # 1.1 å…ˆè¯¢é—® Base URL
    ai_base_url = click.prompt("AI API Base URL", default="https://llmapi.paratera.com", show_default=True)
    config["AI_BASE_URL"] = ai_base_url

    # 1.2 è¯¢é—® API å¯†é’¥
    ai_api_key = click.prompt(
        "è¯·è¾“å…¥ AI API å¯†é’¥ (ç•™ç©ºåˆ™è·³è¿‡ï¼Œç¨åå¯åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ )", default="", show_default=False, hide_input=True
    )
    if ai_api_key:
        config["AI_API_KEY"] = ai_api_key
        # ä½¿ç”¨æä¾›çš„å¯†é’¥æŸ¥è¯¢å¯ç”¨æ¨¡å‹
        available_models = []
        try:
            click.echo("æ­£åœ¨æŸ¥è¯¢å¯ç”¨æ¨¡å‹...")
            client = openai.OpenAI(base_url=ai_base_url, api_key=ai_api_key)
            models_response = client.models.list()
            available_models = [model.id for model in models_response.data]
            click.echo(f"âœ… æ‰¾åˆ° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹")
        except Exception as e:
            click.echo(f"âš ï¸  æ— æ³•æŸ¥è¯¢æ¨¡å‹åˆ—è¡¨: {e}")
            click.echo("   å°†ä½¿ç”¨é»˜è®¤æ¨¡å‹é€‰é¡¹")
            available_models = ["DeepSeek-V3.2-Thinking", "gpt-3.5-turbo", "gpt-4-turbo"]
    else:
        click.echo("âš ï¸  æœªæä¾› API å¯†é’¥ï¼ŒAI æ€»ç»“å’Œç¿»è¯‘åŠŸèƒ½å°†å—é™")
        click.echo("   æ‚¨å¯ä»¥ç¨ååœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ  AI_API_KEY è®¾ç½®")
        config["AI_API_KEY"] = "your_api_key_here"
        available_models = ["DeepSeek-V3.2-Thinking", "gpt-3.5-turbo", "gpt-4-turbo"]

    # 1.3 è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    if available_models:
        click.echo("\nå¯ç”¨æ¨¡å‹åˆ—è¡¨:")

        # æ„å»ºquestionaryé€‰æ‹©é€‰é¡¹
        choices = []
        for model in available_models:
            choices.append(questionary.Choice(title=model, value=model))

        # æ·»åŠ è‡ªå®šä¹‰è¾“å…¥é€‰é¡¹
        choices.append(questionary.Choice(title="[è‡ªå®šä¹‰è¾“å…¥] - è¾“å…¥å…¶ä»–æ¨¡å‹åç§°", value="__custom_input__"))

        # æ˜¾ç¤ºäº¤äº’å¼é€‰æ‹©èœå•
        selected_model = questionary.select(
            "è¯·é€‰æ‹©AIæ¨¡å‹ï¼ˆä½¿ç”¨ä¸Šä¸‹ç®­å¤´å¯¼èˆªï¼Œå›è½¦ç¡®è®¤ï¼‰:", choices=choices, instruction="(ä¸Šä¸‹ç®­å¤´å¯¼èˆªï¼Œå›è½¦ç¡®è®¤)"
        ).ask()

        if selected_model == "__custom_input__":
            # ç”¨æˆ·é€‰æ‹©è‡ªå®šä¹‰è¾“å…¥
            ai_model = click.prompt("è¯·è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°", default="DeepSeek-V3.2-Thinking", show_default=True)
            click.echo(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {ai_model}")
        else:
            ai_model = selected_model
            click.echo(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {ai_model}")
    else:
        ai_model = click.prompt("AI æ¨¡å‹åç§°", default="DeepSeek-V3.2-Thinking", show_default=True)

    config["AI_MODEL"] = ai_model

    # 2. çˆ¬è™«é…ç½®
    click.echo("\nğŸ“Š çˆ¬è™«é…ç½®")
    click.echo("-" * 40)

    max_results_initial = click.prompt("åˆå§‹åŒæ­¥æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§è®ºæ–‡æ•°", default=100, type=int, show_default=True)
    config["MAX_RESULTS_INITIAL"] = str(max_results_initial)

    max_results_daily = click.prompt("æ¯æ—¥åŒæ­¥æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§è®ºæ–‡æ•°", default=20, type=int, show_default=True)
    config["MAX_RESULTS_DAILY"] = str(max_results_daily)

    years_back = click.prompt("åˆå§‹åŒæ­¥å›æº¯çš„å¹´æ•°", default=5, type=int, show_default=True)
    config["YEARS_BACK"] = str(years_back)

    # 3. ç ”ç©¶é¢†åŸŸé€‰æ‹©
    click.echo("\nğŸ¯ é€‰æ‹©æ‚¨çš„ç ”ç©¶é¢†åŸŸ")
    click.echo("-" * 40)
    click.echo("è¯·ä½¿ç”¨ä¸Šä¸‹ç®­å¤´å¯¼èˆªï¼Œç©ºæ ¼é”®é€‰æ‹©/å–æ¶ˆï¼Œå›è½¦ç¡®è®¤ï¼ˆå¯å¤šé€‰ï¼‰ï¼š")

    research_fields = RESEARCH_FIELDS

    # æ„å»ºquestionaryé€‰é¡¹
    choices = []
    for key, field in research_fields.items():
        # ä½¿ç”¨Choiceå¯¹è±¡ï¼ŒåŒ…å«æ ‡é¢˜å’Œæè¿°
        title = f"[{field['name']}] - {field['description']}"
        choices.append(
            questionary.Choice(
                title=title,
                value=key,  # ä¿å­˜å­—æ®µIDç”¨äºåç»­æŸ¥è¯¢
                checked=False,  # é»˜è®¤ä¸é€‰ä¸­
            )
        )

    # æ·»åŠ å…¨é€‰é€‰é¡¹
    choices.insert(0, questionary.Choice(title="[å…¨é€‰] - é€‰æ‹©æ‰€æœ‰ç ”ç©¶é¢†åŸŸ", value="__select_all__", checked=False))

    # æ˜¾ç¤ºäº¤äº’å¼å¤é€‰æ¡†
    selected_keys = questionary.checkbox(
        "è¯·é€‰æ‹©æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶é¢†åŸŸï¼š",
        choices=choices,
        instruction="(ç©ºæ ¼é”®åˆ‡æ¢é€‰æ‹©ï¼Œå›è½¦ç¡®è®¤)",
        validate=lambda selected: len(selected) > 0 or "è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç ”ç©¶é¢†åŸŸ",
    ).ask()

    if not selected_keys:
        click.echo("âŒ æœªé€‰æ‹©ä»»ä½•ç ”ç©¶é¢†åŸŸï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
        selected_keys = ["condensed_matter", "dft", "machine_learning"]

    selected_queries = []
    selected_field_names = []

    # å¤„ç†é€‰æ‹©
    if "__select_all__" in selected_keys:
        # é€‰æ‹©å…¨éƒ¨ï¼ˆæ’é™¤å…¨é€‰æ ‡è®°ï¼‰
        for field in research_fields.values():
            selected_queries.append(field["query"])
            selected_field_names.append(field["name"])
        click.echo("âœ… å·²é€‰æ‹©å…¨éƒ¨ç ”ç©¶é¢†åŸŸ")
    else:
        # å¤„ç†ç”¨æˆ·é€‰æ‹©
        for key in selected_keys:
            if key in research_fields:
                field = research_fields[key]
                selected_queries.append(field["query"])
                selected_field_names.append(field["name"])
                click.echo(f"âœ… å·²é€‰æ‹©: {field['name']}")
            else:
                click.echo(f"âš ï¸  æœªçŸ¥çš„é¢†åŸŸID: {key}")

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªé€‰æ‹©
    if not selected_queries:
        click.echo("âš ï¸  æœªé€‰æ‹©ä»»ä½•é¢†åŸŸï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        selected_queries = [
            research_fields["condensed_matter"]["query"],
            research_fields["dft"]["query"],
            research_fields["machine_learning"]["query"],
        ]
        selected_field_names = [
            research_fields["condensed_matter"]["name"],
            research_fields["dft"]["name"],
            research_fields["machine_learning"]["name"],
        ]

    config["SEARCH_QUERIES"] = "; ".join(selected_queries)
    config["_SELECTED_FIELD_NAMES"] = selected_field_names

    # 3.5 æ™ºèƒ½å»ºè®®ï¼ˆåŸºäºé€‰æ‹©çš„é¢†åŸŸæ•°é‡ï¼‰
    num_selected_fields = len(selected_field_names)
    click.echo(f"\nğŸ“Š æ™ºèƒ½å»ºè®®ï¼ˆåŸºäºæ‚¨é€‰æ‹©çš„ {num_selected_fields} ä¸ªç ”ç©¶é¢†åŸŸï¼‰")
    click.echo("-" * 40)

    # æ ¹æ®é¢†åŸŸæ•°é‡æä¾›å»ºè®®
    recommended_initial = 100
    recommended_daily = 20

    if num_selected_fields <= 3:
        click.echo("âœ… æ‚¨é€‰æ‹©äº†å°‘é‡é¢†åŸŸï¼Œä¿æŒé»˜è®¤é…ç½®å³å¯ã€‚")
    elif num_selected_fields <= 6:
        recommended_initial = 70
        recommended_daily = 15
        click.echo(f"âš ï¸  æ‚¨é€‰æ‹©äº†ä¸­ç­‰æ•°é‡é¢†åŸŸï¼Œå»ºè®®è°ƒæ•´çˆ¬è™«é…ç½®ä»¥é¿å…è¿‡å¤šè®ºæ–‡ï¼š")
        click.echo(f"   - åˆå§‹åŒæ­¥æ¯ä¸ªæŸ¥è¯¢æœ€å¤§è®ºæ–‡æ•°: {recommended_initial} (åŸé»˜è®¤: 100)")
        click.echo(f"   - æ¯æ—¥åŒæ­¥æ¯ä¸ªæŸ¥è¯¢æœ€å¤§è®ºæ–‡æ•°: {recommended_daily} (åŸé»˜è®¤: 20)")
    else:
        recommended_initial = 50
        recommended_daily = 10
        click.echo(f"âš ï¸  æ‚¨é€‰æ‹©äº†å¤§é‡é¢†åŸŸ ({num_selected_fields}ä¸ª)ï¼Œå¼ºçƒˆå»ºè®®è°ƒæ•´çˆ¬è™«é…ç½®ï¼š")
        click.echo(f"   - åˆå§‹åŒæ­¥æ¯ä¸ªæŸ¥è¯¢æœ€å¤§è®ºæ–‡æ•°: {recommended_initial} (åŸé»˜è®¤: 100)")
        click.echo(f"   - æ¯æ—¥åŒæ­¥æ¯ä¸ªæŸ¥è¯¢æœ€å¤§è®ºæ–‡æ•°: {recommended_daily} (åŸé»˜è®¤: 20)")
        click.echo(f"   - æ³¨æ„ï¼šåŒæ­¥å¤§é‡é¢†åŸŸå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’Œæ›´å¤šå­˜å‚¨ç©ºé—´ã€‚")

    # è¯¢é—®ç”¨æˆ·æ˜¯å¦åº”ç”¨å»ºè®®
    if num_selected_fields > 3:
        if click.confirm("\nğŸ’¡ æ˜¯å¦åº”ç”¨ä¸Šè¿°å»ºè®®è°ƒæ•´çˆ¬è™«é…ç½®ï¼Ÿ", default=True):
            config["MAX_RESULTS_INITIAL"] = str(recommended_initial)
            config["MAX_RESULTS_DAILY"] = str(recommended_daily)
            click.echo(
                f"âœ… å·²åº”ç”¨å»ºè®®é…ç½®ï¼šMAX_RESULTS_INITIAL={recommended_initial}, MAX_RESULTS_DAILY={recommended_daily}"
            )
        else:
            click.echo("â„¹ï¸  ä¿æŒæ‚¨åŸæœ‰çš„çˆ¬è™«é…ç½®ã€‚")

    # 4. æŠ¥å‘Šé…ç½®
    click.echo("\nğŸ“„ æŠ¥å‘Šé…ç½®")
    click.echo("-" * 40)

    report_max_papers = click.prompt("æ¯ä»½æŠ¥å‘Šæ˜¾ç¤ºçš„æœ€å¤§è®ºæ–‡æ•°", default=50, type=int, show_default=True)
    config["REPORT_MAX_PAPERS"] = str(report_max_papers)

    summary_sentences_limit = click.prompt("æ‘˜è¦å¥å­æ•°é™åˆ¶", default=3, type=int, show_default=True)
    config["SUMMARY_SENTENCES_LIMIT"] = str(summary_sentences_limit)

    click.echo("\nâœ… é…ç½®å®Œæˆï¼")
    return config, int(years_back)


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--years-back", type=int, default=None, help="åˆå§‹åŒæ­¥å›æº¯çš„å¹´æ•°ï¼ˆé»˜è®¤ï¼šäº¤äº’å¼é…ç½®ï¼‰")
def init(directory, years_back):
    """åˆå§‹åŒ–ç›®å½•å¹¶åŒæ­¥å†å²è®ºæ–‡"""
    directory = Path(directory).resolve()

    # åˆ›å»ºç›®å½•ç»“æ„
    (directory / "data").mkdir(exist_ok=True)
    (directory / "reports").mkdir(exist_ok=True)
    (directory / "logs").mkdir(exist_ok=True)

    # åˆ›å»º .env æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    env_file = directory / ".env"
    custom_banner_fields = None  # ç”¨äºå­˜å‚¨è‡ªå®šä¹‰æ¨ªå¹…å­—æ®µ

    if not env_file.exists():
        # äº¤äº’å¼é…ç½®
        config, interactive_years_back = interactive_configuration()

        # ä¿å­˜è‡ªå®šä¹‰æ¨ªå¹…å­—æ®µ
        custom_banner_fields = config.get("_SELECTED_FIELD_NAMES", [])

        # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ²¡æœ‰æŒ‡å®š years_backï¼Œä½¿ç”¨äº¤äº’å¼é…ç½®çš„å€¼
        if years_back is None:
            years_back = interactive_years_back

        # ç”Ÿæˆ .env æ–‡ä»¶å†…å®¹
        env_content = f"""# arXiv Pulse é…ç½®æ–‡ä»¶
# ç”±äº¤äº’å¼é…ç½®å‘å¯¼äº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")} ç”Ÿæˆ

# ========================
# AI API é…ç½® (æ”¯æŒ OpenAI æ ¼å¼)
# ========================
AI_API_KEY={config.get("AI_API_KEY", "your_api_key_here")}
AI_MODEL={config.get("AI_MODEL", "DeepSeek-V3.2-Thinking")}
AI_BASE_URL={config.get("AI_BASE_URL", "https://llmapi.paratera.com")}

# ========================
# æ•°æ®åº“é…ç½®
# ========================
DATABASE_URL=sqlite:///data/arxiv_papers.db

# ========================
# çˆ¬è™«é…ç½®
# ========================
MAX_RESULTS_INITIAL={config.get("MAX_RESULTS_INITIAL", "100")}    # initå‘½ä»¤æ¯ä¸ªæŸ¥è¯¢çš„è®ºæ–‡æ•°
MAX_RESULTS_DAILY={config.get("MAX_RESULTS_DAILY", "20")}        # syncå‘½ä»¤æ¯ä¸ªæŸ¥è¯¢çš„è®ºæ–‡æ•°

# ========================
# æœç´¢æŸ¥è¯¢é…ç½®
# ========================
# åˆ†å·åˆ†éš”ï¼Œå…è®¸æŸ¥è¯¢ä¸­åŒ…å«é€—å·
# æ ¹æ®æ‚¨çš„é€‰æ‹©ç”Ÿæˆçš„ç ”ç©¶é¢†åŸŸæŸ¥è¯¢
SEARCH_QUERIES={config.get("SEARCH_QUERIES", 'condensed matter physics AND cat:cond-mat.*; (ti:"density functional" OR abs:"density functional") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci OR cat:physics.chem-ph); (ti:"machine learning" OR abs:"machine learning") AND (cat:physics.comp-ph OR cat:cond-mat.mtrl-sci OR cat:physics.chem-ph)')}

# ========================
# æŠ¥å‘Šé…ç½®
# ========================
REPORT_DIR=reports
SUMMARY_MAX_TOKENS=2000          # æ€»ç»“å’Œç¿»è¯‘çš„æœ€å¤§tokenæ•°
SUMMARY_SENTENCES_LIMIT={config.get("SUMMARY_SENTENCES_LIMIT", "3")}
TOKEN_PRICE_PER_MILLION=3.0
REPORT_MAX_PAPERS={config.get("REPORT_MAX_PAPERS", "50")}

# ========================
# åŒæ­¥é…ç½®
# ========================
YEARS_BACK={config.get("YEARS_BACK", "3")}               # åŒæ­¥å›æº¯çš„å¹´æ•°
IMPORTANT_PAPERS_FILE=important_papers.txt

# ========================
# å¯é€‰é…ç½®
# ========================
# æ—¥å¿—çº§åˆ«: DEBUG, INFO, WARNING, ERROR (é»˜è®¤: INFO)
LOG_LEVEL=INFO

# çˆ¬è™«å»¶è¿Ÿï¼ˆç§’ï¼Œé¿å…é¢‘ç¹è¯·æ±‚ arXiv APIï¼‰
CRAWL_DELAY=1.0
"""

        env_file.write_text(env_content)
        click.echo(f"\nâœ… å·²åœ¨ {directory} åˆ›å»º .env é…ç½®æ–‡ä»¶")

    else:
        click.echo(f".env æ–‡ä»¶å·²å­˜åœ¨äº {directory}")
        if years_back is None:
            years_back = 5  # é»˜è®¤å€¼

    # åˆ›å»º important_papers.txtï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    important_file = directory / "important_papers.txt"
    if not important_file.exists():
        important_file.write_text("# åœ¨æ­¤æ·»åŠ é‡è¦è®ºæ–‡çš„arXiv IDï¼Œæ¯è¡Œä¸€ä¸ª\n")
        click.echo(f"âœ… å·²åœ¨ {directory} åˆ›å»º important_papers.txt æ–‡ä»¶")

    # è®¾ç½®ç¯å¢ƒå¹¶éªŒè¯é…ç½®
    if not setup_environment(directory):
        click.echo("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        sys.exit(1)

    # ç¡®è®¤åŒæ­¥
    click.echo("\n" + "=" * 60)
    click.echo("å‡†å¤‡åŒæ­¥æ•°æ®åº“")
    click.echo("=" * 60)
    click.echo(f"å³å°†å¼€å§‹åˆå§‹åŒæ­¥ï¼Œå›æº¯ {years_back} å¹´å†å²è®ºæ–‡...")
    click.echo(f"è¿™å¯èƒ½ä¼šèŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ‚¨é€‰æ‹©çš„é¢†åŸŸæ•°é‡ã€‚")
    click.echo(f"æ‚¨å¯ä»¥åœ¨ä»»ä½•æ—¶å€™æŒ‰ Ctrl+C ä¸­æ–­åŒæ­¥ã€‚")

    if not click.confirm("\nğŸš€ ç¡®è®¤å¼€å§‹åŒæ­¥æ•°æ®åº“å—ï¼Ÿ", default=True):
        click.echo("âŒ å·²å–æ¶ˆåŒæ­¥")
        sys.exit(0)

    click.echo(f"\nâ³ å¼€å§‹åˆå§‹åŒæ­¥ï¼Œå›æº¯ {years_back} å¹´å†å²è®ºæ–‡...")
    sync_result = sync_papers(years_back=years_back, summarize=False)

    # ç”Ÿæˆæ¨ªå¹…æ ‡é¢˜
    if custom_banner_fields:
        banner_title = custom_banner_fields[:4]  # é™åˆ¶æœ€å¤š4ä¸ªå­—æ®µ
    else:
        banner_title = generate_banner_title(env_file)
    print_banner_custom(banner_title)

    click.echo(f"\nğŸ‰ arXiv Pulse åˆå§‹åŒ–å®Œæˆï¼")
    click.echo(f"\nğŸ“ æ–‡ä»¶ä½ç½®ï¼š")
    click.echo(f"  é…ç½®æ–‡ä»¶: {env_file}")
    click.echo(f"  æ•°æ®åº“: {directory}/data/arxiv_papers.db")
    click.echo(f"  æŠ¥å‘Šç›®å½•: {directory}/reports/")
    click.echo(f"\nğŸš€ ä¸‹ä¸€æ­¥ï¼š")
    click.echo(f"  1. è¿è¡Œ 'pulse sync {directory}' æ›´æ–°æœ€æ–°è®ºæ–‡")
    click.echo(f"  2. è¿è¡Œ 'pulse search \"å…³é”®è¯\" {directory}' æœç´¢è®ºæ–‡")
    click.echo(f"  3. è¿è¡Œ 'pulse recent {directory}' æŸ¥çœ‹æœ€è¿‘è®ºæ–‡æŠ¥å‘Š")
    click.echo(f"  4. ç¼–è¾‘ {important_file} æ·»åŠ é‡è¦è®ºæ–‡")


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--years-back", type=int, default=None, help="åŒæ­¥å›æº¯çš„å¹´æ•°ï¼ˆé»˜è®¤ï¼šå¼ºåˆ¶æ¨¡å¼5å¹´ï¼Œæ™®é€šæ¨¡å¼1å¹´ï¼‰")
@click.option("--summarize/--no-summarize", default=False, help="æ˜¯å¦æ€»ç»“æ–°è®ºæ–‡ï¼ˆé»˜è®¤ï¼šå¦ï¼‰")
@click.option("--force", is_flag=True, default=False, help="å¼ºåˆ¶åŒæ­¥ï¼šé‡æ–°ä¸‹è½½æœ€è¿‘Nå¹´çš„æ‰€æœ‰è®ºæ–‡ï¼Œå¿½ç•¥é‡å¤æ£€æŸ¥")
def sync(directory, years_back, summarize, force):
    """åŒæ­¥æœ€æ–°è®ºæ–‡åˆ°æ•°æ®åº“

    å¼ºåˆ¶æ¨¡å¼(--force): é‡æ–°ä¸‹è½½æœ€è¿‘Nå¹´çš„æ‰€æœ‰è®ºæ–‡ï¼Œå¿½ç•¥é‡å¤æ£€æŸ¥ï¼Œé»˜è®¤å›æº¯5å¹´ã€‚
    æ™®é€šæ¨¡å¼: åªä¸‹è½½ç¼ºå¤±çš„æ–°è®ºæ–‡ï¼Œé»˜è®¤å›æº¯1å¹´ã€‚
    """
    directory = Path(directory).resolve()
    click.echo(f"æ­£åœ¨åŒæ­¥ arXiv Pulse äº {directory}")

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    # è®¾ç½®é»˜è®¤years_backå€¼
    if years_back is None:
        years_back = 5 if force else 1
        click.echo(f"ä½¿ç”¨é»˜è®¤å›æº¯å¹´æ•°: {years_back} å¹´")

    # åŒæ­¥è®ºæ–‡
    sync_result = sync_papers(years_back=years_back, summarize=summarize, force=force)

    click.echo("\n" + "=" * 50)
    click.echo("åŒæ­¥å®Œæˆï¼æ•°æ®åº“å·²æ›´æ–°ã€‚")


@cli.command()
@click.argument("query")
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--limit", default=20, help="è¿”å›ç»“æœçš„æœ€å¤§æ•°é‡ï¼ˆé»˜è®¤ï¼š20ï¼‰")
@click.option("--years-back", type=int, default=0, help="æœç´¢å‰åŒæ­¥å›æº¯çš„å¹´æ•°ï¼ˆé»˜è®¤ï¼š0ï¼Œä¸æ›´æ–°ï¼‰")
@click.option("--use-ai/--no-ai", default=True, help="æ˜¯å¦ä½¿ç”¨AIç†è§£è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆé»˜è®¤ï¼šæ˜¯ï¼‰")
@click.option("--summarize/--no-summarize", default=True, help="æ˜¯å¦è‡ªåŠ¨æ€»ç»“æœªæ€»ç»“çš„è®ºæ–‡ï¼ˆé»˜è®¤ï¼šæ˜¯ï¼‰")
@click.option("--max-summarize", type=int, default=0, help="æœ€å¤§æ€»ç»“è®ºæ–‡æ•°ï¼ˆé»˜è®¤ï¼š0è¡¨ç¤ºæ— é™åˆ¶ï¼‰")
@click.option("--categories", "-c", multiple=True, help="åŒ…å«çš„åˆ†ç±»ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰")
@click.option("--days-back", type=int, help="å›æº¯å¤©æ•°ï¼ˆä¾‹å¦‚ï¼š30è¡¨ç¤ºæœ€è¿‘30å¤©ï¼‰")
@click.option("--authors", "-a", multiple=True, help="ä½œè€…å§“åï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰")
@click.option(
    "--sort-by",
    type=click.Choice(["published", "relevance_score", "title", "updated"]),
    default="published",
    help="æ’åºå­—æ®µ",
)
def search(
    query, directory, limit, years_back, use_ai, summarize, max_summarize, categories, days_back, authors, sort_by
):
    """æ™ºèƒ½æœç´¢è®ºæ–‡ï¼ˆæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢å’ŒåŸºæœ¬è¿‡æ»¤ï¼‰"""
    directory = Path(directory).resolve()

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    # å¦‚æœéœ€è¦ï¼Œå…ˆåŒæ­¥æœ€æ–°è®ºæ–‡
    crawler = ArXivCrawler()
    if years_back > 0:
        click.echo(f"æœç´¢å‰å…ˆåŒæ­¥æœ€è¿‘ {years_back} å¹´è®ºæ–‡...")
        sync_result = sync_papers(years_back=years_back, summarize=False, force=False)
        crawler = sync_result["crawler"]

    click.echo(f"\næ­£åœ¨æœç´¢: '{query}'")
    click.echo("=" * 50)

    search_terms = [query]

    # å¦‚æœå¯ç”¨AIä¸”é…ç½®äº†AI APIå¯†é’¥ï¼Œå°è¯•è§£æè‡ªç„¶è¯­è¨€æŸ¥è¯¢
    if use_ai and Config.AI_API_KEY:
        try:
            import openai

            client = openai.OpenAI(api_key=Config.AI_API_KEY, base_url=Config.AI_BASE_URL)

            ai_prompt = f"""
            ç”¨æˆ·æ­£åœ¨æœç´¢arXivç‰©ç†/è®¡ç®—ææ–™ç§‘å­¦è®ºæ–‡ï¼ŒæŸ¥è¯¢æ˜¯: "{query}"
            
            è¯·å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºé€‚åˆarXivæœç´¢çš„å…³é”®è¯æˆ–çŸ­è¯­ã€‚
            
            é‡è¦è§„åˆ™ï¼š
            1. å¦‚æœæŸ¥è¯¢å·²ç»æ˜¯æ˜ç¡®çš„æœç´¢è¯ï¼ˆå¦‚"DeepH"ã€"deep learning Hamiltonian"ã€"DFTè®¡ç®—"ï¼‰ï¼Œç›´æ¥ä½¿ç”¨å®ƒï¼Œä¸è¦æ·»åŠ åŒä¹‰è¯
            2. å¦‚æœæŸ¥è¯¢åŒ…å«ä¸“ä¸šæœ¯è¯­ã€ç¼©å†™æˆ–ä¸“æœ‰åè¯ï¼Œä¿æŒåŸæ ·ä½œä¸ºä¸»è¦æœç´¢è¯
            3. ä»…å½“æŸ¥è¯¢éå¸¸æ¨¡ç³Šæˆ–ä¸€èˆ¬æ€§æ—¶ï¼ˆå¦‚"æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦ä¸­çš„åº”ç”¨"ï¼‰ï¼Œæ‰ç”Ÿæˆ1-2ä¸ªç›¸å…³å…³é”®è¯
            4. ä¼˜å…ˆä¿æŒæŸ¥è¯¢çš„åŸå§‹æ„å›¾ï¼Œä¸è¦æ·»åŠ ä¸ç›¸å…³çš„å…³é”®è¯
            5. å¯¹äºè‹±æ–‡æŸ¥è¯¢ï¼Œä¿æŒåŸæ ·ï¼›å¯¹äºä¸­æ–‡æŸ¥è¯¢ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡å…³é”®è¯
            è€ƒè™‘ä»¥ä¸‹é¢†åŸŸï¼šå‡èšæ€ç‰©ç†ã€å¯†åº¦æ³›å‡½ç†è®º(DFT)ã€æœºå™¨å­¦ä¹ ã€åŠ›åœºã€åˆ†å­åŠ¨åŠ›å­¦ã€é‡å­åŒ–å­¦ã€è®¡ç®—ææ–™ç§‘å­¦ã€‚
            
            è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼ŒåŒ…å«1-2ä¸ªæœç´¢å…³é”®è¯/çŸ­è¯­ã€‚
            ç¤ºä¾‹ï¼š
            - æŸ¥è¯¢"DeepH": ["DeepH"]
            - æŸ¥è¯¢"deep learning Hamiltonian": ["deep learning Hamiltonian"]
            - æŸ¥è¯¢"DFTè®¡ç®—": ["DFT"]
            - æŸ¥è¯¢"åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ": ["molecular dynamics simulation"]
            - æŸ¥è¯¢"æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦ä¸­çš„åº”ç”¨": ["machine learning materials science"]
            
            åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡æœ¬ã€‚
            """

            response = client.chat.completions.create(
                model=Config.AI_MODEL,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯arXivè®ºæ–‡æœç´¢åŠ©æ‰‹ï¼Œæ“…é•¿è¯†åˆ«ä¸“ä¸šæœ¯è¯­å¹¶å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå­¦æœ¯æœç´¢å…³é”®è¯ã€‚",
                    },
                    {"role": "user", "content": ai_prompt},
                ],
                max_tokens=200,
                temperature=0.3,
            )

            ai_response = response.choices[0].message.content
            try:
                search_terms = json.loads(ai_response)
                if isinstance(search_terms, list) and len(search_terms) > 0:
                    click.echo(f"AIè§£æçš„æœç´¢è¯: {', '.join(search_terms[:3])}")
                    if len(search_terms) > 3:
                        click.echo(f"  ä»¥åŠ {len(search_terms) - 3} ä¸ªå…¶ä»–å…³é”®è¯")
            except:
                # å¦‚æœAIå“åº”ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                pass

        except Exception as e:
            click.echo(f"AIè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {e}")

    # åœ¨æ•°æ®åº“ä¸­æœç´¢
    with crawler.db.get_session() as session:
        from arxiv_pulse.models import Paper

        # ä½¿ç”¨å¢å¼ºæœç´¢å¼•æ“è¿›è¡Œæ¨¡ç³Šæœç´¢
        search_engine = SearchEngine(session)

        # å°†æœç´¢è¯åˆå¹¶ä¸ºä¸€ä¸ªæŸ¥è¯¢ï¼ˆæœç´¢å¼•æ“ä¼šå¤„ç†å•è¯æ‹†åˆ†å’ŒåŒä¹‰è¯æ‰©å±•ï¼‰
        combined_query = " ".join(search_terms)

        filter_config = SearchFilter(
            query=combined_query,
            search_fields=["title", "abstract"],
            categories=list(categories) if categories else None,
            authors=list(authors) if authors else None,
            author_match="contains",  # é»˜è®¤ä½¿ç”¨åŒ…å«åŒ¹é…
            days_back=days_back,
            limit=limit * min(len(search_terms), 2),  # æ‰©å¤§é™åˆ¶ä½†æœ€å¤š2å€ï¼Œé¿å…è¿‡å¤šç»“æœ
            sort_by=sort_by,
            sort_order="desc",
            match_all=True,  # ANDé€»è¾‘ï¼šåŒ¹é…æ‰€æœ‰æœç´¢è¯
        )

        # æ‰§è¡Œæœç´¢
        papers_to_show = search_engine.search_papers(filter_config)

        # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
        papers_to_show = papers_to_show[:limit]

        click.echo(f"æ‰¾åˆ° {len(papers_to_show)} ç¯‡è®ºæ–‡:")

        # ç”Ÿæˆæœç´¢æŠ¥å‘Š
        click.echo("æ­£åœ¨ç”Ÿæˆæœç´¢æŠ¥å‘Š...")
        files = generate_search_report(
            query,
            search_terms,
            papers_to_show,
            paper_limit=limit,
            summarize=summarize,
            max_summarize=max_summarize,
        )

        # è¾“å‡ºç®€è¦ç»“æœå’ŒæŠ¥å‘Šæ–‡ä»¶
        for i, paper in enumerate(papers_to_show[:5], 1):  # åªæ˜¾ç¤ºå‰5ç¯‡ä½œä¸ºé¢„è§ˆ
            authors = json.loads(paper.authors) if paper.authors else []
            author_names = [a.get("name", "") for a in authors[:2]]
            if len(authors) > 2:
                author_names.append("ç­‰")

            click.echo(f"\n{i}. {paper.title}")
            click.echo(f"   ä½œè€…: {', '.join(author_names)}")
            click.echo(f"   arXiv ID: {paper.arxiv_id}")
            click.echo(f"   å‘å¸ƒæ—¥æœŸ: {paper.published.strftime('%Y-%m-%d') if paper.published else 'N/A'}")

        if len(papers_to_show) > 5:
            click.echo(f"\n... ä»¥åŠ {len(papers_to_show) - 5} ç¯‡æ›´å¤šè®ºæ–‡")

        click.echo(f"\næŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š")
        for f in files:
            click.echo(f"  - {f}")
        click.echo(f"\nè¯¦ç»†è®ºæ–‡ä¿¡æ¯ã€ä¸­æ–‡ç¿»è¯‘å’ŒPDFé“¾æ¥è¯·æŸ¥çœ‹ç”Ÿæˆçš„MarkdownæŠ¥å‘Šã€‚")


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
@click.option("--limit", default=50, help="æŠ¥å‘Šä¸­åŒ…å«çš„æœ€å¤§è®ºæ–‡æ•°ï¼ˆé»˜è®¤ï¼š50ï¼‰")
@click.option("--days-back", type=int, default=2, help="åŒ…å«æœ€è¿‘å¤šå°‘å¤©çš„è®ºæ–‡ï¼ˆé»˜è®¤ï¼š2å¤©ï¼‰")
@click.option("--years-back", type=int, default=1, help="æŠ¥å‘Šå‰åŒæ­¥å›æº¯çš„å¹´æ•°ï¼ˆé»˜è®¤ï¼š1å¹´ï¼‰")
@click.option("--summarize/--no-summarize", default=True, help="æ˜¯å¦è‡ªåŠ¨æ€»ç»“æœªæ€»ç»“çš„è®ºæ–‡ï¼ˆé»˜è®¤ï¼šæ˜¯ï¼‰")
@click.option("--max-summarize", type=int, default=0, help="æœ€å¤§æ€»ç»“è®ºæ–‡æ•°ï¼ˆé»˜è®¤ï¼š0è¡¨ç¤ºæ— é™åˆ¶ï¼‰")
def recent(directory, limit, days_back, years_back, summarize, max_summarize):
    """ç”Ÿæˆæœ€è¿‘è®ºæ–‡çš„æŠ¥å‘Šï¼ˆå…ˆåŒæ­¥æœ€æ–°è®ºæ–‡ï¼‰"""
    directory = Path(directory).resolve()

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    # å…ˆåŒæ­¥è®ºæ–‡
    if years_back > 0:
        click.echo(f"æŠ¥å‘Šå‰å…ˆåŒæ­¥æœ€è¿‘ {years_back} å¹´è®ºæ–‡...")
        sync_papers(years_back=years_back, summarize=False, force=False)

    # ç”ŸæˆæŠ¥å‘Š
    click.echo("\n" + "=" * 50)
    click.echo(f"æ­£åœ¨ç”Ÿæˆæœ€è¿‘ {days_back} å¤©è®ºæ–‡æŠ¥å‘Š...")

    files = generate_report(paper_limit=limit, days_back=days_back, summarize=summarize, max_summarize=max_summarize)

    click.echo(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š")
    for f in files:
        click.echo(f"  - {f}")


@cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False), default=".")
def stat(directory):
    """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    directory = Path(directory).resolve()

    if not setup_environment(directory):
        sys.exit(1)

    print_banner()

    crawler = ArXivCrawler()
    summarizer = PaperSummarizer()
    report_generator = ReportGenerator()

    click.echo("\n" + "=" * 50)
    click.echo("arXiv Pulse æ•°æ®åº“ç»Ÿè®¡")
    click.echo("=" * 50)

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    crawl_stats = crawler.get_crawler_stats()
    summary_stats = summarizer.get_summary_stats()

    # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡
    click.echo(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    click.echo(f"   æ€»è®ºæ–‡æ•°: {crawl_stats['total_papers']}")
    click.echo(f"   ä»Šæ—¥è®ºæ–‡: {crawl_stats['papers_today']}")
    click.echo(f"   å·²æ€»ç»“è®ºæ–‡: {summary_stats['summarized_papers']}")
    click.echo(f"   æ€»ç»“ç‡: {summary_stats['summarization_rate']:.1%}")

    # æŒ‰æœç´¢æŸ¥è¯¢ç»Ÿè®¡
    click.echo(f"\nğŸ” æŒ‰æœç´¢æŸ¥è¯¢åˆ†å¸ƒ:")
    for query, count in crawl_stats["papers_by_query"].items():
        percentage = count / crawl_stats["total_papers"] * 100 if crawl_stats["total_papers"] > 0 else 0
        click.echo(f"   {query}: {count} ç¯‡ ({percentage:.1f}%)")

    # åˆ†ç±»ç»Ÿè®¡
    click.echo(f"\nğŸ“ åˆ†ç±»ç»Ÿè®¡:")
    with crawler.db.get_session() as session:
        from arxiv_pulse.models import Paper
        import json

        papers = session.query(Paper).all()
        category_counts = {}

        for paper in papers:
            if paper.categories:
                for cat in paper.categories.split():
                    category_counts[cat] = category_counts.get(cat, 0) + 1

        # æŒ‰æ•°é‡æ’åº
        sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            percentage = count / crawl_stats["total_papers"] * 100 if crawl_stats["total_papers"] > 0 else 0
            click.echo(f"   {category}: {count} ç¯‡ ({percentage:.1f}%)")

        if len(sorted_categories) > 10:
            click.echo(f"   ... ä»¥åŠ {len(sorted_categories) - 10} ä¸ªå…¶ä»–åˆ†ç±»")

    # æ—¶é—´åˆ†å¸ƒ
    click.echo(f"\nğŸ“… æ—¶é—´åˆ†å¸ƒ:")
    with crawler.db.get_session() as session:
        # æŒ‰å¹´ç»Ÿè®¡
        year_stats = {}
        for paper in papers:
            if paper.published:
                year = paper.published.year
                year_stats[year] = year_stats.get(year, 0) + 1

        sorted_years = sorted(year_stats.items())
        for year, count in sorted_years[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5å¹´
            percentage = count / crawl_stats["total_papers"] * 100 if crawl_stats["total_papers"] > 0 else 0
            click.echo(f"   {year}å¹´: {count} ç¯‡ ({percentage:.1f}%)")

    # æ€»ç»“ç»Ÿè®¡
    pending_papers = crawl_stats["total_papers"] - summary_stats["summarized_papers"]
    click.echo(f"\nğŸ¤– AIæ€»ç»“ç»Ÿè®¡:")
    click.echo(f"   å·²æ€»ç»“: {summary_stats['summarized_papers']} ç¯‡")
    click.echo(f"   å¾…æ€»ç»“: {pending_papers} ç¯‡")
    click.echo(f"   æ€»ç»“ç‡: {summary_stats['summarization_rate']:.1%}")

    click.echo("\n" + "=" * 50)
    click.echo("ç»Ÿè®¡å®Œæˆ âœ…")


if __name__ == "__main__":
    cli()
